{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Last_One.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzJCg2UUnWUu",
        "colab_type": "code",
        "outputId": "4b1879c4-ee55-4cc3-a82b-5bf0837ea8a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.14\n",
        "#!pip install tensorflow-gpu==2.0.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==1.14 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.1.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.17.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.33.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.14.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.11.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14) (42.0.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81sxpU6gc0xF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf \"162935\"\n",
        "# !rm -rf \"Data/validate\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQzbpxR2nkIb",
        "colab_type": "code",
        "outputId": "f98b48d3-3ae0-447e-f4d0-570c3ffec24d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "!pip install mne"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mne\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/7c/ad1b52a3fdd4be8f55e183f1eff7d76f48cd1bee83c5630f9c26770e032e/mne-0.19.2-py3-none-any.whl (6.4MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4MB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from mne) (1.17.5)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mne) (1.4.1)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-0.19.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saFYJSR8W4ZH",
        "colab_type": "code",
        "outputId": "12680971-0755-4504-8e1a-bf1ab2b371e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "#sys.path.append('/content/drive/My Drive/Brain_Data')\n",
        "#%cd \"My Drive/Brain_Data\"\n",
        "!ls\n",
        "#%cd drive\n",
        "#!ls\n",
        "%cd \"/content/drive/My Drive/Brain_Data\"\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "drive  sample_data\n",
            "/content/drive/My Drive/Brain_Data\n",
            " CallbackCascadeMulti.py\n",
            " callback_EEGNet.py\n",
            " capsnet_log.csv\n",
            " Data\n",
            " data_download.py\n",
            " data_utils_binary.py\n",
            " data_utils_EEGNet.py\n",
            " data_utils.py\n",
            " EEGModels.py\n",
            " EEGNet_data\n",
            " Experiments\n",
            "'Experiments\\Testmy_model.h5'\n",
            " Input_Capsule_Network.ipynb\n",
            " Last_One.ipynb\n",
            " layers.py\n",
            " logs\n",
            " memory_raw.fif\n",
            " ModelCascadeMulti.py\n",
            " model.h5\n",
            " model.hdf5\n",
            " model.png\n",
            " MultiPatient_MultiTaskClassification_EEGNet.ipynb\n",
            " parallel_callback.py\n",
            " parallel_generate_data.py\n",
            " parallel_model_working.py\n",
            " parallel_multi_model.py\n",
            " __pycache__\n",
            " reading_raw.py\n",
            " resting_raw.fif\n",
            " run_download_batch_subjects.py\n",
            " subjects\n",
            " tensorboard-logs\n",
            " Testmy_model.h5\n",
            " Testmy_model_weights_depth_5.h5\n",
            " Testmy_model_weights.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m1Q9YR_Yyqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Jan 12 23:24:04 2020\n",
        "\n",
        "@author: Smail\n",
        "\"\"\"\n",
        "import os\n",
        "import boto3\n",
        "import os.path as op\n",
        "import reading_raw\n",
        "import mne\n",
        "import shutil\n",
        "import h5py\n",
        "\n",
        "\n",
        "def get_raw_data(subject, type_state, hcp_path):\n",
        "#   if(subject != \"133019\" or subject !=\"164636\" or subject != \"105923\" or subject !=\"162935\"):\n",
        "#     print(\"subject not among the list, aborting operation ...\")\n",
        "#     return\n",
        "  try: #type of state for this subject might not exist\n",
        "    print(\"Reading the binary file and returning the raw matrix ...\")\n",
        "    raw = reading_raw.read_raw(subject=subject, hcp_path=hcp_path, run_index=0, data_type=type_state)\n",
        "    raw.load_data()\n",
        "    meg_picks = mne.pick_types(raw.info, meg=True, ref_meg=False)\n",
        "    raw_matrix = raw[meg_picks[:]][0]\n",
        "    del raw\n",
        "    return raw_matrix\n",
        "  except Exception as e:\n",
        "    print(\"Problem in reading the file: The type of state '{}' might not be there for subject '{}'\".format(type_state, subject))\n",
        "    print(\"Exception error : \",e)\n",
        "    return False\n",
        "\n",
        "def create_data_directory():\n",
        "    print(\"Creating data directory or skipping if already existing...\")\n",
        "    if(not os.path.isdir(\"Data\")):\n",
        "        try:\n",
        "            os.mkdir(\"Data\") \n",
        "            print(\"Created Data folder !\")\n",
        "        except Exception as e:\n",
        "            print (\"Creation of the Data directory failed\")\n",
        "            print(\"Exception error: \",str(e))\n",
        "            \n",
        "    if(not os.path.isdir(\"Data/train\")):\n",
        "        try:\n",
        "            os.mkdir(\"Data/train\")    \n",
        "            print(\"Created train folder !\")\n",
        "        except Exception as e:\n",
        "            print (\"Creation of the train directory failed\")\n",
        "            print(\"Exception error: \",str(e))\n",
        "            \n",
        "    if(not os.path.isdir(\"Data/validate\")):\n",
        "        try:\n",
        "            os.mkdir(\"Data/validate\")  \n",
        "            print(\"Created validate folder !\")\n",
        "        except Exception as e:\n",
        "            print (\"Creation of the validate directory failed\")\n",
        "            print(\"Exception error: \",str(e))\n",
        "            \n",
        "    if(not os.path.isdir(\"Data/test\")):\n",
        "        try:\n",
        "            os.mkdir(\"Data/test\")    \n",
        "            print(\"Created test folder !\")\n",
        "        except Exception as e:\n",
        "            print (\"Creation of the test directory failed\")\n",
        "            print(\"Exception error: \",str(e))\n",
        "            \n",
        "def closestNumber(n, m) : \n",
        "    q = int(n / m) \n",
        "    n1 = m * q \n",
        "    if((n * m) > 0) : \n",
        "        n2 = (m * (q + 1))  \n",
        "    else : \n",
        "        n2 = (m * (q - 1)) \n",
        "    if (abs(n - n1) < abs(n - n2)) : \n",
        "        return n1 \n",
        "    return n2 \n",
        "\n",
        "\n",
        "def download_subject(subject,personal_access_key_id,secret_access_key):\n",
        "    \n",
        "  s3 = boto3.client('s3', aws_access_key_id=personal_access_key_id, aws_secret_access_key=secret_access_key)\n",
        "\n",
        "  folders = [\"3-Restin\",\"4-Restin\",\"5-Restin\",\"6-Wrkmem\",\"7-Wrkmem\",\"8-StoryM\",\"9-StoryM\",\"10-Motort\",\"11-Motort\"]\n",
        "  #filename_test = [\"e,rfhp1.0Hz,COH\"]\n",
        "  filenames = [\"c,rfDC\", \"config\", \"e,rfhp1.0Hz,COH\", \"e,rfhp1.0Hz,COH1\"]\n",
        "\n",
        "  print(\"Creating the directories for the subject '{}'\".format(subject))\n",
        "  print()\n",
        "  if op.exists(os.getcwd()+\"//\"+subject) == False:\n",
        "    for folder in folders:\n",
        "        os.makedirs(subject+\"/unprocessed/MEG/\"+folder+\"/4D/\")\n",
        "  print(\"done !\")\n",
        "  print()\n",
        "  print(\"Will start downloading the following files for all folders:\")\n",
        "  print(filenames)\n",
        "  print()\n",
        "  print()\n",
        "  #print(filename_test)\n",
        "  for filename in filenames:\n",
        "    for folder in folders:\n",
        "      if filename == \"c,rfDC\":\n",
        "        print(\"downloading c,rfDC file for folder {} ...\".format(folder))\n",
        "        print()\n",
        "      if(op.exists(os.getcwd()+\"//\"+subject+'/unprocessed/MEG/'+folder+'/4D/'+filename)):\n",
        "        print(\"File already exists, moving on ...\")\n",
        "        print()\n",
        "        pass\n",
        "      try:\n",
        "        s3.download_file('hcp-openaccess', 'HCP_1200/'+subject+'/unprocessed/MEG/'+folder+'/4D/'+filename, subject+'/unprocessed/MEG/'+folder+'/4D/'+filename)\n",
        "        if filename == \"c,rfDC\":\n",
        "          print(\"done downloading c,rfDC for folder {} !\".format(folder))\n",
        "          print()\n",
        "      except Exception as e:\n",
        "        print()\n",
        "        print(\"the folder '{}' for subject '{}' does not exist in Amazon server, moving to next folder ...\".format(folder,subject))\n",
        "        print(\"Exception error message: \"+str(e))\n",
        "        pass\n",
        "    \n",
        "    \n",
        "\n",
        "def create_h5_files(raw_matrix,subject,type_state):\n",
        "    print()\n",
        "    print(\"shape of raw matrix\",raw_matrix.shape)\n",
        "    print()\n",
        "    train_folder = \"Data/train/\"\n",
        "    validate_folder = \"Data/validate/\"\n",
        "    test_folder = \"Data/test/\"\n",
        "    \n",
        "    number_columns_rest = 181*1425 #257,925\n",
        "    number_columns_mem = 325*1425 #463,125\n",
        "    number_columns_math = 234*1425 #333,450\n",
        "    number_columns_motor = 363*1425 #517,275\n",
        "    \n",
        "    test_subject_name = \"162026\"\n",
        "    # if (subject != \"162935\"):\n",
        "\n",
        "    number_columns = 250 * 1425\n",
        "\n",
        "\n",
        "    if(type_state == \"rest\"): #we divide the file by 12 parts\n",
        "        if (subject != test_subject_name):\n",
        "          number_columns_per_chunk = number_columns // 10\n",
        "          for i in range(10,20):\n",
        "            if i >= 10 and i < 18:\n",
        "                destination_file = train_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
        "            if i >= 18 and i < 20:\n",
        "                destination_file = validate_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
        "            start_index_col = number_columns_per_chunk * i\n",
        "            stop_index_col = start_index_col + number_columns_per_chunk - 1\n",
        "            with h5py.File(destination_file, \"w\") as hf:\n",
        "                hf.create_dataset(type_state+'_'+subject, data=raw_matrix[ : , start_index_col : stop_index_col ],compression=\"gzip\", compression_opts=4)\n",
        "        else:\n",
        "          number_columns_per_chunk = number_columns // 10\n",
        "          for i in range(10,20):\n",
        "            start_index_col = number_columns_per_chunk * i\n",
        "            stop_index_col = start_index_col + number_columns_per_chunk - 1 \n",
        "            destination_file = test_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
        "            with h5py.File(destination_file, \"w\") as hf:\n",
        "                    hf.create_dataset(type_state+'_'+subject, data=raw_matrix[ : , start_index_col : stop_index_col ],compression=\"gzip\", compression_opts=4)\n",
        "          \n",
        "          \n",
        "    \n",
        "    if(type_state == \"task_working_memory\"): #we divide the file by 24 parts\n",
        "        if (subject != test_subject_name):\n",
        "          number_columns_per_chunk = number_columns // 10\n",
        "          for i in range(10,20): # we choose only 5 chunks of this data to solve data imbalance\n",
        "            if i>= 10 and i < 18:\n",
        "                destination_file = train_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
        "            if i >= 18 and i < 20:\n",
        "                destination_file = validate_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
        "            start_index_col = number_columns_per_chunk * i\n",
        "            stop_index_col = start_index_col + number_columns_per_chunk - 1\n",
        "            with h5py.File(destination_file, \"w\") as hf:\n",
        "                hf.create_dataset(type_state+'_'+subject, data=raw_matrix[ : , start_index_col : stop_index_col ],compression=\"gzip\", compression_opts=4) # lossless compression :) , 4 is the best option\n",
        "        else:\n",
        "          number_columns_per_chunk = number_columns // 10\n",
        "          for i in range(10,20):\n",
        "            start_index_col = number_columns_per_chunk * i\n",
        "            stop_index_col = start_index_col + number_columns_per_chunk - 1\n",
        "            destination_file = test_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
        "            with h5py.File(destination_file, \"w\") as hf:\n",
        "                    hf.create_dataset(type_state+'_'+subject, data=raw_matrix[ : , start_index_col : stop_index_col ],compression=\"gzip\", compression_opts=4)\n",
        "           \n",
        "        \n",
        "    if(type_state == \"task_story_math\"): #we divide the file by 18 parts\n",
        "        if (subject != test_subject_name):\n",
        "          number_columns_per_chunk = number_columns // 10\n",
        "          for i in range(10,20): # we choose only 5 chunks of this data to solve data imbalance\n",
        "            if i >= 10 and i < 18:\n",
        "                destination_file = train_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
        "            if i >= 18 and i < 20:\n",
        "                destination_file = validate_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
        "            start_index_col = number_columns_per_chunk * i\n",
        "            stop_index_col = start_index_col + number_columns_per_chunk - 1\n",
        "            with h5py.File(destination_file, \"w\") as hf:\n",
        "                hf.create_dataset(type_state+'_'+subject, data=raw_matrix[ : , start_index_col : stop_index_col ],compression=\"gzip\", compression_opts=4)\n",
        "        else:\n",
        "          number_columns_per_chunk = number_columns // 10\n",
        "          for i in range(10,20):\n",
        "            start_index_col = number_columns_per_chunk * i\n",
        "            stop_index_col = start_index_col + number_columns_per_chunk - 1\n",
        "            destination_file = test_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
        "            with h5py.File(destination_file, \"w\") as hf:\n",
        "                    hf.create_dataset(type_state+'_'+subject, data=raw_matrix[ : , start_index_col : stop_index_col ],compression=\"gzip\", compression_opts=4)\n",
        "    \n",
        "    if(type_state == \"task_motor\"): #we divide the file by 30 parts\n",
        "        if (subject != test_subject_name):\n",
        "          number_columns_per_chunk = number_columns // 10\n",
        "          for i in range(10,20):\n",
        "            if i >= 10 and i < 18:\n",
        "                destination_file = train_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
        "            if i >= 18 and i < 20:\n",
        "                destination_file = validate_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
        "            start_index_col = number_columns_per_chunk * i\n",
        "            stop_index_col = start_index_col + number_columns_per_chunk - 1\n",
        "            with h5py.File(destination_file, \"w\") as hf:\n",
        "                hf.create_dataset(type_state+'_'+subject, data=raw_matrix[ : , start_index_col : stop_index_col ],compression=\"gzip\", compression_opts=4)\n",
        "        else:\n",
        "          number_columns_per_chunk = number_columns // 10\n",
        "          for i in range(10,20):\n",
        "            start_index_col = number_columns_per_chunk * i\n",
        "            stop_index_col = start_index_col + number_columns_per_chunk - 1\n",
        "            destination_file = test_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
        "            with h5py.File(destination_file, \"w\") as hf:\n",
        "                hf.create_dataset(type_state+'_'+subject, data=raw_matrix[ : , start_index_col : stop_index_col ],compression=\"gzip\", compression_opts=4)\n",
        "          \n",
        "    \n",
        "\n",
        "#Main function to be executed to download subjects\n",
        "#list_subjects should be a list of strings containing the 6 digits subjects\n",
        "#hcp_path should be the current working directory (os.getcwd())\n",
        "def download_batch_subjects(list_subjects, personal_access_key_id, secret_access_key, hcp_path): # hcp_path should be os.getcwd()\n",
        "  create_data_directory()  \n",
        "  state_types = [\"rest\", \"task_working_memory\", \"task_story_math\", \"task_motor\"]\n",
        "  for subject in list_subjects:\n",
        "    download_subject(subject,personal_access_key_id,secret_access_key)\n",
        "    for state in state_types:\n",
        "      matrix_raw = get_raw_data(subject, state, hcp_path)\n",
        "      if type(matrix_raw) != type(False): # if the reading was done successfully\n",
        "        print()\n",
        "        print(\"Creating the compressed h5 files ...\")\n",
        "        create_h5_files(matrix_raw,subject,state)\n",
        "    print(\"done creating the compressed h5 files for subject '{}' !\".format(subject))\n",
        "    print()\n",
        "    print(\"deleting the directory containing the binary files of subject '{}' ...\".format(subject))\n",
        "    print()\n",
        "    try:\n",
        "      shutil.rmtree(subject+\"/\",ignore_errors=True)#Removes the folder and all folders/files inside\n",
        "      print(\"Done deleting the directory of the binary files!\")\n",
        "      print(\"Moving on to the next subject ...\")\n",
        "      print()\n",
        "    except Exception as e :\n",
        "      print()\n",
        "      print(\"Error while trying to delete the directory.\")\n",
        "      print(\"Exception message : \" + str(e))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heNN5BeZ4LDP",
        "colab_type": "code",
        "outputId": "9325ac6d-450b-46f7-a0a3-3326b40bf217",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# import custom_data_download as utils\n",
        "import os\n",
        "#subjects = ['100307', '102816', '105923', '106521', '108323', '109123', '111514', '112920', '113922', '116726', '133019', '140117', '146129']\n",
        "# subjects_ismail = ['105923']\n",
        "\n",
        "# subjects_ismail = ['164636','105923','162935']\n",
        "# subjects_ismail = ['133019','164636']\n",
        "# subjects_ismail = ['164636']\n",
        "# subjects_ismail = ['105923','162935']\n",
        "subjects_ismail = ['108323','105923'] \n",
        "\n",
        "\n",
        "\n",
        "#subjects_ismail = ['100307','102816','105923' ]\n",
        "personal_access_key_id = \"AKIAXO65CT57MAX2EWPN\"\n",
        "secret_access_key = \"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\"\n",
        "\n",
        "create_data_directory()\n",
        "# subjects_ismail = utils.get_filtered_subjects(personal_access_key_id,secret_access_key,subjects_ismail)\n",
        "download_batch_subjects(subjects_ismail, personal_access_key_id, secret_access_key, os.getcwd())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating data directory or skipping if already existing...\n",
            "Creating data directory or skipping if already existing...\n",
            "Creating the directories for the subject '108323'\n",
            "\n",
            "done !\n",
            "\n",
            "Will start downloading the following files for all folders:\n",
            "['c,rfDC', 'config', 'e,rfhp1.0Hz,COH', 'e,rfhp1.0Hz,COH1']\n",
            "\n",
            "\n",
            "downloading c,rfDC file for folder 3-Restin ...\n",
            "\n",
            "done downloading c,rfDC for folder 3-Restin !\n",
            "\n",
            "downloading c,rfDC file for folder 4-Restin ...\n",
            "\n",
            "done downloading c,rfDC for folder 4-Restin !\n",
            "\n",
            "downloading c,rfDC file for folder 5-Restin ...\n",
            "\n",
            "done downloading c,rfDC for folder 5-Restin !\n",
            "\n",
            "downloading c,rfDC file for folder 6-Wrkmem ...\n",
            "\n",
            "done downloading c,rfDC for folder 6-Wrkmem !\n",
            "\n",
            "downloading c,rfDC file for folder 7-Wrkmem ...\n",
            "\n",
            "done downloading c,rfDC for folder 7-Wrkmem !\n",
            "\n",
            "downloading c,rfDC file for folder 8-StoryM ...\n",
            "\n",
            "done downloading c,rfDC for folder 8-StoryM !\n",
            "\n",
            "downloading c,rfDC file for folder 9-StoryM ...\n",
            "\n",
            "done downloading c,rfDC for folder 9-StoryM !\n",
            "\n",
            "downloading c,rfDC file for folder 10-Motort ...\n",
            "\n",
            "done downloading c,rfDC for folder 10-Motort !\n",
            "\n",
            "downloading c,rfDC file for folder 11-Motort ...\n",
            "\n",
            "done downloading c,rfDC for folder 11-Motort !\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/108323/unprocessed/MEG/3-Restin/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 765918  =      0.000 ...   376.463 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 765919)\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/108323/unprocessed/MEG/6-Wrkmem/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 1264283  =      0.000 ...   621.419 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 1264284)\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/108323/unprocessed/MEG/8-StoryM/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 951663  =      0.000 ...   467.760 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 951664)\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/108323/unprocessed/MEG/10-Motort/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 1419781  =      0.000 ...   697.849 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 1419782)\n",
            "\n",
            "done creating the compressed h5 files for subject '108323' !\n",
            "\n",
            "deleting the directory containing the binary files of subject '108323' ...\n",
            "\n",
            "Done deleting the directory of the binary files!\n",
            "Moving on to the next subject ...\n",
            "\n",
            "Creating the directories for the subject '105923'\n",
            "\n",
            "done !\n",
            "\n",
            "Will start downloading the following files for all folders:\n",
            "['c,rfDC', 'config', 'e,rfhp1.0Hz,COH', 'e,rfhp1.0Hz,COH1']\n",
            "\n",
            "\n",
            "downloading c,rfDC file for folder 3-Restin ...\n",
            "\n",
            "done downloading c,rfDC for folder 3-Restin !\n",
            "\n",
            "downloading c,rfDC file for folder 4-Restin ...\n",
            "\n",
            "done downloading c,rfDC for folder 4-Restin !\n",
            "\n",
            "downloading c,rfDC file for folder 5-Restin ...\n",
            "\n",
            "done downloading c,rfDC for folder 5-Restin !\n",
            "\n",
            "downloading c,rfDC file for folder 6-Wrkmem ...\n",
            "\n",
            "done downloading c,rfDC for folder 6-Wrkmem !\n",
            "\n",
            "downloading c,rfDC file for folder 7-Wrkmem ...\n",
            "\n",
            "done downloading c,rfDC for folder 7-Wrkmem !\n",
            "\n",
            "downloading c,rfDC file for folder 8-StoryM ...\n",
            "\n",
            "done downloading c,rfDC for folder 8-StoryM !\n",
            "\n",
            "downloading c,rfDC file for folder 9-StoryM ...\n",
            "\n",
            "done downloading c,rfDC for folder 9-StoryM !\n",
            "\n",
            "downloading c,rfDC file for folder 10-Motort ...\n",
            "\n",
            "done downloading c,rfDC for folder 10-Motort !\n",
            "\n",
            "downloading c,rfDC file for folder 11-Motort ...\n",
            "\n",
            "done downloading c,rfDC for folder 11-Motort !\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/105923/unprocessed/MEG/3-Restin/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 749069  =      0.000 ...   368.182 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 749070)\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/105923/unprocessed/MEG/6-Wrkmem/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 1270779  =      0.000 ...   624.612 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 1270780)\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/105923/unprocessed/MEG/8-StoryM/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 948009  =      0.000 ...   465.964 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 948010)\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/105923/unprocessed/MEG/10-Motort/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 1476215  =      0.000 ...   725.587 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 1476216)\n",
            "\n",
            "done creating the compressed h5 files for subject '105923' !\n",
            "\n",
            "deleting the directory containing the binary files of subject '105923' ...\n",
            "\n",
            "Done deleting the directory of the binary files!\n",
            "Moving on to the next subject ...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLARChcZFm3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "training_file_dir = \"Data/train\"\n",
        "all_train_files = [f for f in listdir(training_file_dir) if isfile(join(training_file_dir, f))]\n",
        "train_files_dirs = []\n",
        "for i in range(len(all_train_files)):\n",
        "    train_files_dirs.append(training_file_dir+'/'+all_train_files[i])\n",
        "\n",
        "validation_file_dir = \"Data/validate\"\n",
        "all_validate_files = [f for f in listdir(validation_file_dir) if isfile(join(validation_file_dir, f))]\n",
        "validate_files_dirs = []\n",
        "for i in range(len(all_validate_files)):\n",
        "    validate_files_dirs.append(validation_file_dir+'/'+all_validate_files[i])\n",
        "\n",
        "\n",
        "\n",
        "# print(train_files_dirs)\n",
        "# print(validate_files_dirs)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29Q-zulREJu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Jan 11 23:44:54 2020\n",
        "\n",
        "@author: Smail\n",
        "\"\"\"\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "# import data_utils as utils\n",
        "import gc\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "import h5py\n",
        "from scipy import stats\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "def normalize_zscore(matrix):\n",
        "    return stats.zscore(matrix)\n",
        "\n",
        "\n",
        "\n",
        "def separate_list(all_files_list):\n",
        "    task_list = []\n",
        "    rest_list = []\n",
        "    for item in all_files_list:\n",
        "        if \"task\" in item:\n",
        "            task_list.append(item)\n",
        "        else:\n",
        "            rest_list.append(item)\n",
        "\n",
        "    return task_list, rest_list\n",
        "\n",
        "def orderer_shuffling(rest_list,task_list):\n",
        "    ordered_list = []\n",
        "    for index, (value1, value2) in enumerate(zip(rest_list, task_list)):\n",
        "        ordered_list.append(value1)\n",
        "        ordered_list.append(value2)\n",
        "    if(len(rest_list) != len(task_list)):\n",
        "        len_rest = len(rest_list)\n",
        "        len_task = len(task_list)\n",
        "        diff_len = abs(len_rest-len_task)\n",
        "        if(len(rest_list) > len(task_list)):\n",
        "            ordered_list.extend(rest_list[-diff_len:])\n",
        "        else:\n",
        "            ordered_list.extend(task_list[-diff_len:])\n",
        "    return ordered_list\n",
        "\n",
        "def get_lists_indexes(matrix_length):\n",
        "        indexes_1 = np.arange(start=0,stop = matrix_length-4,step=5)\n",
        "        indexes_2 = np.arange(start=1,stop = matrix_length-3,step=5)\n",
        "        indexes_3 = np.arange(start=2,stop = matrix_length-2,step=5)\n",
        "        indexes_4 = np.arange(start=3,stop = matrix_length-1,step=5)\n",
        "        indexes_5 = np.arange(start=4,stop = matrix_length-0,step=5)\n",
        "        return (indexes_1,indexes_2,indexes_3,indexes_4,indexes_5)\n",
        "    \n",
        "def get_input_lists(matrix,indexes):\n",
        "    inputs = []\n",
        "    for i in range(5):\n",
        "        inputs.append(np.take(matrix,indexes[i],axis=0))\n",
        "    del matrix\n",
        "    return inputs[0],inputs[1],inputs[2],inputs[3],inputs[4]\n",
        "\n",
        "def closestNumber(n, m) : \n",
        "    q = int(n / m) \n",
        "    n1 = m * q \n",
        "    if((n * m) > 0) : \n",
        "        n2 = (m * (q + 1))  \n",
        "    else : \n",
        "        n2 = (m * (q - 1)) \n",
        "    if (abs(n - n1) < abs(n - n2)) : \n",
        "        return n1 \n",
        "    return n2 \n",
        "\n",
        "\n",
        "\n",
        "def normalize_matrix(matrix):\n",
        "    max,min = matrix.max(),matrix.min()\n",
        "    return (matrix-min)/(max-min)\n",
        "\n",
        "def get_dataset_name(file_name_with_dir):\n",
        "    filename_without_dir = file_name_with_dir.split('/')[-1]\n",
        "    temp = filename_without_dir.split('_')[:-1]\n",
        "    dataset_name = \"_\".join(temp)\n",
        "    return dataset_name\n",
        "\n",
        "def array_to_mesh(arr):    \n",
        "    assert arr.shape == (1,248),\"the shape of the input array should be (1,248) because there are 248 MEG channels,received array of shape \" + str(arr.shape)\n",
        "    output = np.zeros((20,22),dtype = np.float)\n",
        "    \n",
        "    #121\n",
        "    output[0][10] = arr[0][120]\n",
        "      \n",
        "    #89\n",
        "    output[1][12] = arr[0][151]\n",
        "    output[1][11] = arr[0][119]\n",
        "    output[1][10] = arr[0][88]\n",
        "    output[1][9] = arr[0][89]\n",
        "    output[1][8] = arr[0][121]\n",
        "    \n",
        "    #61\n",
        "    output[2][13] = arr[0][150]\n",
        "    output[2][12] = arr[0][118]\n",
        "    output[2][11] = arr[0][87]\n",
        "    output[2][10] = arr[0][60]\n",
        "    output[2][9] = arr[0][61]\n",
        "    output[2][8] = arr[0][90]\n",
        "    output[2][7] = arr[0][122]\n",
        "    \n",
        "    #37\n",
        "    output[3][14] = arr[0][149]\n",
        "    output[3][13] = arr[0][117]\n",
        "    output[3][12] = arr[0][86]\n",
        "    output[3][11] = arr[0][59]\n",
        "    output[3][10] = arr[0][36]\n",
        "    output[3][9] = arr[0][37]\n",
        "    output[3][8] = arr[0][62]\n",
        "    output[3][7] = arr[0][91]\n",
        "    output[3][6] = arr[0][123]\n",
        "    \n",
        "    #19\n",
        "    output[4][17] = arr[0][194]\n",
        "    output[4][16] = arr[0][175]\n",
        "    output[4][15] = arr[0][148]\n",
        "    output[4][14] = arr[0][116]\n",
        "    output[4][13] = arr[0][85]\n",
        "    output[4][12] = arr[0][58]\n",
        "    output[4][11] = arr[0][35]\n",
        "    output[4][10] = arr[0][18]\n",
        "    output[4][9] = arr[0][19]\n",
        "    output[4][8] = arr[0][38]\n",
        "    output[4][7] = arr[0][63]\n",
        "    output[4][6] = arr[0][92]\n",
        "    output[4][5] = arr[0][152]\n",
        "    output[4][4] = arr[0][176]\n",
        "\n",
        "    #5\n",
        "    output[5][20] = arr[0][247]\n",
        "    output[5][19] = arr[0][227]\n",
        "    output[5][18] = arr[0][193]\n",
        "    output[5][17] = arr[0][174]\n",
        "    output[5][16] = arr[0][147]\n",
        "    output[5][15] = arr[0][115]\n",
        "    output[5][14] = arr[0][84]\n",
        "    output[5][13] = arr[0][57]\n",
        "    output[5][12] = arr[0][34]\n",
        "    output[5][11] = arr[0][17]\n",
        "    output[5][10] = arr[0][4]\n",
        "    output[5][9] = arr[0][5]\n",
        "    output[5][8] = arr[0][20]\n",
        "    output[5][7] = arr[0][39]\n",
        "    output[5][6] = arr[0][64]\n",
        "    output[5][5] = arr[0][93]\n",
        "    output[5][4] = arr[0][125]\n",
        "    output[5][3] = arr[0][153]\n",
        "    output[5][2] = arr[0][177]\n",
        "    output[5][1] = arr[0][211]\n",
        "    output[5][0] = arr[0][228]\n",
        "    \n",
        "    #4\n",
        "    output[6][20] = arr[0][246]\n",
        "    output[6][19] = arr[0][226]\n",
        "    output[6][18] = arr[0][192]\n",
        "    output[6][17] = arr[0][173]\n",
        "    output[6][16] = arr[0][146]\n",
        "    output[6][15] = arr[0][114]\n",
        "    output[6][14] = arr[0][83]\n",
        "    output[6][13] = arr[0][56]\n",
        "    output[6][12] = arr[0][33]\n",
        "    output[6][11] = arr[0][16]\n",
        "    output[6][10] = arr[0][3]\n",
        "    output[6][9] = arr[0][6]\n",
        "    output[6][8] = arr[0][21]\n",
        "    output[6][7] = arr[0][40]\n",
        "    output[6][6] = arr[0][65]\n",
        "    output[6][5] = arr[0][94]\n",
        "    output[6][4] = arr[0][126]\n",
        "    output[6][3] = arr[0][154]\n",
        "    output[6][2] = arr[0][178]\n",
        "    output[6][1] = arr[0][212]\n",
        "    output[6][0] = arr[0][229]\n",
        "\n",
        "    \n",
        "    #3\n",
        "    output[7][19] = arr[0][245]\n",
        "    output[7][18] = arr[0][210]\n",
        "    output[7][17] = arr[0][172]\n",
        "    output[7][16] = arr[0][145]\n",
        "    output[7][15] = arr[0][113]\n",
        "    output[7][14] = arr[0][82]\n",
        "    output[7][13] = arr[0][55]\n",
        "    output[7][12] = arr[0][32]\n",
        "    output[7][11] = arr[0][15]\n",
        "    output[7][10] = arr[0][2]\n",
        "    output[7][9] = arr[0][7]\n",
        "    output[7][8] = arr[0][22]\n",
        "    output[7][7] = arr[0][41]\n",
        "    output[7][6] = arr[0][66]\n",
        "    output[7][5] = arr[0][95]\n",
        "    output[7][4] = arr[0][127]\n",
        "    output[7][3] = arr[0][155]\n",
        "    output[7][2] = arr[0][195]\n",
        "    output[7][1] = arr[0][230]\n",
        "            \n",
        "    #8\n",
        "    output[8][19] = arr[0][244]\n",
        "    output[8][18] = arr[0][209]\n",
        "    output[8][17] = arr[0][171]\n",
        "    output[8][16] = arr[0][144]\n",
        "    output[8][15] = arr[0][112]\n",
        "    output[8][14] = arr[0][81]\n",
        "    output[8][13] = arr[0][54]\n",
        "    output[8][12] = arr[0][31]\n",
        "    output[8][11] = arr[0][14]\n",
        "    output[8][10] = arr[0][1]\n",
        "    output[8][9] = arr[0][8]\n",
        "    output[8][8] = arr[0][23]\n",
        "    output[8][7] = arr[0][42]\n",
        "    output[8][6] = arr[0][67]\n",
        "    output[8][5] = arr[0][96]\n",
        "    output[8][4] = arr[0][128]\n",
        "    output[8][3] = arr[0][156]\n",
        "    output[8][2] = arr[0][196]\n",
        "    output[8][1] = arr[0][231]\n",
        "    \n",
        "    #1\n",
        "    output[9][19] = arr[0][243]\n",
        "    output[9][18] = arr[0][208]\n",
        "    output[9][17] = arr[0][170]\n",
        "    output[9][16] = arr[0][143]\n",
        "    output[9][15] = arr[0][111]\n",
        "    output[9][14] = arr[0][80]\n",
        "    output[9][13] = arr[0][53]\n",
        "    output[9][12] = arr[0][30]\n",
        "    output[9][11] = arr[0][13]\n",
        "    output[9][10] = arr[0][0]\n",
        "    output[9][9] = arr[0][9]\n",
        "    output[9][8] = arr[0][24]\n",
        "    output[9][7] = arr[0][43]\n",
        "    output[9][6] = arr[0][68]\n",
        "    output[9][5] = arr[0][97]\n",
        "    output[9][4] = arr[0][129]\n",
        "    output[9][3] = arr[0][157]\n",
        "    output[9][2] = arr[0][197]\n",
        "    output[9][1] = arr[0][232]\n",
        "    \n",
        "    #12\n",
        "    output[10][18] = arr[0][225]\n",
        "    output[10][17] = arr[0][191]\n",
        "    output[10][16] = arr[0][142]\n",
        "    output[10][15] = arr[0][110]\n",
        "    output[10][14] = arr[0][79]\n",
        "    output[10][13] = arr[0][52]\n",
        "    output[10][12] = arr[0][29]\n",
        "    output[10][11] = arr[0][12]\n",
        "    output[10][10] = arr[0][11]\n",
        "    output[10][9] = arr[0][10]\n",
        "    output[10][8] = arr[0][25]\n",
        "    output[10][7] = arr[0][44]\n",
        "    output[10][6] = arr[0][69]\n",
        "    output[10][5] = arr[0][98]\n",
        "    output[10][4] = arr[0][130]\n",
        "    output[10][3] = arr[0][179]\n",
        "    output[10][2] = arr[0][213]\n",
        "    \n",
        "    #28\n",
        "    output[11][16] = arr[0][169]\n",
        "    output[11][15] = arr[0][141]\n",
        "    output[11][14] = arr[0][109]\n",
        "    output[11][13] = arr[0][78]\n",
        "    output[11][12] = arr[0][51]\n",
        "    output[11][11] = arr[0][28]\n",
        "    output[11][10] = arr[0][27]\n",
        "    output[11][9] = arr[0][26]\n",
        "    output[11][8] = arr[0][45]\n",
        "    output[11][7] = arr[0][70]\n",
        "    output[11][6] = arr[0][99]\n",
        "    output[11][5] = arr[0][131]\n",
        "    output[11][4] = arr[0][158]\n",
        "    \n",
        "    #49\n",
        "    output[12][17] = arr[0][190]\n",
        "    output[12][16] = arr[0][168]\n",
        "    output[12][15] = arr[0][140]\n",
        "    output[12][14] = arr[0][108]\n",
        "    output[12][13] = arr[0][77]\n",
        "    output[12][12] = arr[0][50]\n",
        "    output[12][11] = arr[0][49]\n",
        "    output[12][10] = arr[0][48]\n",
        "    output[12][9] = arr[0][47]\n",
        "    output[12][8] = arr[0][46]\n",
        "    output[12][7] = arr[0][71]\n",
        "    output[12][6] = arr[0][100]\n",
        "    output[12][5] = arr[0][132]\n",
        "    output[12][4] = arr[0][159]\n",
        "    output[12][3] = arr[0][180]\n",
        "\n",
        "    \n",
        "    #75\n",
        "    output[13][18] = arr[0][224]\n",
        "    output[13][17] = arr[0][207]\n",
        "    output[13][16] = arr[0][189]\n",
        "    output[13][15] = arr[0][167]\n",
        "    output[13][14] = arr[0][139]\n",
        "    output[13][13] = arr[0][107]\n",
        "    output[13][12] = arr[0][76]\n",
        "    output[13][11] = arr[0][75]\n",
        "    output[13][10] = arr[0][74]\n",
        "    output[13][9] = arr[0][73]\n",
        "    output[13][8] = arr[0][72]\n",
        "    output[13][7] = arr[0][101]\n",
        "    output[13][6] = arr[0][133]\n",
        "    output[13][5] = arr[0][160]\n",
        "    output[13][4] = arr[0][181]\n",
        "    output[13][3] = arr[0][198]\n",
        "    output[13][2] = arr[0][214]\n",
        "    \n",
        "    #105\n",
        "    output[14][18] = arr[0][242]\n",
        "    output[14][17] = arr[0][223]\n",
        "    output[14][16] = arr[0][206]\n",
        "    output[14][15] = arr[0][188]\n",
        "    output[14][14] = arr[0][166]\n",
        "    output[14][13] = arr[0][138]\n",
        "    output[14][12] = arr[0][106]\n",
        "    output[14][11] = arr[0][105]\n",
        "    output[14][10] = arr[0][104]\n",
        "    output[14][9] = arr[0][103]\n",
        "    output[14][8] = arr[0][102]\n",
        "    output[14][7] = arr[0][134]\n",
        "    output[14][6] = arr[0][161]\n",
        "    output[14][5] = arr[0][182]\n",
        "    output[14][4] = arr[0][199]\n",
        "    output[14][3] = arr[0][215]\n",
        "    output[14][2] = arr[0][233]\n",
        "    \n",
        "    \n",
        "    #137\n",
        "    output[15][16] = arr[0][241]\n",
        "    output[15][15] = arr[0][222]\n",
        "    output[15][14] = arr[0][205]\n",
        "    output[15][13] = arr[0][187]\n",
        "    output[15][12] = arr[0][165]\n",
        "    output[15][11] = arr[0][137]\n",
        "    output[15][10] = arr[0][136]\n",
        "    output[15][9] = arr[0][135]\n",
        "    output[15][8] = arr[0][162]\n",
        "    output[15][7] = arr[0][183]\n",
        "    output[15][6] = arr[0][200]\n",
        "    output[15][5] = arr[0][216]\n",
        "    output[15][4] = arr[0][234]\n",
        "    \n",
        "    \n",
        "    #mix\n",
        "    output[16][15] = arr[0][240]\n",
        "    output[16][14] = arr[0][221]\n",
        "    output[16][13] = arr[0][204]\n",
        "    output[16][12] = arr[0][186]\n",
        "    output[16][11] = arr[0][164]\n",
        "    output[16][10] = arr[0][163]\n",
        "    output[16][9] = arr[0][184]\n",
        "    output[16][8] = arr[0][201]\n",
        "    output[16][7] = arr[0][217]\n",
        "    output[16][6] = arr[0][235]\n",
        "   \n",
        "    #186\n",
        "    output[17][12] = arr[0][220]\n",
        "    output[17][11] = arr[0][203]\n",
        "    output[17][10] = arr[0][185]\n",
        "    output[17][9] = arr[0][202]\n",
        "    output[17][8] = arr[0][218]\n",
        "   \n",
        "    #220\n",
        "    output[18][11] = arr[0][239]\n",
        "    output[18][10] = arr[0][219]\n",
        "    output[18][9] = arr[0][236]\n",
        "    \n",
        "    #mix\n",
        "    output[19][11] = arr[0][238]\n",
        "    output[19][10] = arr[0][237]\n",
        "    \n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_data_type(matrix):\n",
        "    # matrix = matrix * 1e11\n",
        "    # matrix = matrix * 1e21\n",
        "    matrix = normalize_matrix(matrix)\n",
        "\n",
        "    if(matrix.shape[1] == 1):\n",
        "        length = 1\n",
        "    else:\n",
        "        length = closestNumber(int(matrix.shape[1]) - 5,5)\n",
        "        \n",
        "    meshes = np.zeros((length,20,22),dtype=np.float64)\n",
        "    for i in range(length):\n",
        "        array_time_step = np.reshape(matrix[:,i],(1,248))\n",
        "        meshes[i] = array_to_mesh(array_time_step)\n",
        "\n",
        "    del matrix\n",
        "\n",
        "    input1,input2,input3,input4,input5 = get_input_lists(meshes, get_lists_indexes(length))\n",
        "\n",
        "    del meshes\n",
        "\n",
        "    number_y_labels = int(length/5)\n",
        "    y_rest = np.ones((number_y_labels,1),dtype=np.int8)\n",
        "    return (input1,input2,input3,input4,input5), y_rest\n",
        "\n",
        "def load_data(file_dirs):\n",
        "    rest_matrix = np.random.rand(248,1)\n",
        "    math_matrix = np.random.rand(248,1)\n",
        "    memory_matrix = np.random.rand(248,1)\n",
        "    motor_matrix = np.random.rand(248,1)\n",
        "    number_classes = 4\n",
        "    # files_paths = file_dirs\n",
        "    index_batch_files = 0\n",
        "    number_files_per_batch = 2\n",
        "    # print(files_paths)\n",
        "\n",
        "\n",
        "    # files_to_load = files_paths[index_batch_files * number_files_per_batch: (index_batch_files + 1) * number_files_per_batch]\n",
        "    # print(files_to_load)\n",
        "    files_to_load = file_dirs\n",
        "    print(\"number of files to load: {}\".format(len(files_to_load)))\n",
        "    for i in range(len(files_to_load)):\n",
        "        # print(i)\n",
        "        if \"rest\" in files_to_load[i]:\n",
        "            with h5py.File(files_to_load[i],'r') as f:\n",
        "                dataset_name = get_dataset_name(files_to_load[i])\n",
        "                print(dataset_name)\n",
        "                matrix = f.get(dataset_name)\n",
        "                matrix = np.array(matrix)\n",
        "            print(matrix.shape)\n",
        "            assert matrix.shape[0] == 248 , \"This rest data does not have 248 channels, but {} instead\".format(matrix.shape[0])\n",
        "            rest_matrix = np.column_stack((rest_matrix, matrix))\n",
        "\n",
        "        if \"math\" in files_to_load[i]:\n",
        "            with h5py.File(files_to_load[i],'r') as f:\n",
        "                dataset_name = get_dataset_name(files_to_load[i])\n",
        "                matrix = f.get(dataset_name)\n",
        "                matrix = np.array(matrix)\n",
        "            print(matrix.shape)\n",
        "            assert matrix.shape[0] == 248 , \"This math data does not have 248 channels, but {} instead\".format(matrix.shape[0])\n",
        "            math_matrix = np.column_stack((math_matrix, matrix))\n",
        "            \n",
        "        if \"memory\" in files_to_load[i]:\n",
        "            with h5py.File(files_to_load[i],'r') as f:\n",
        "                dataset_name = get_dataset_name(files_to_load[i])\n",
        "                matrix = f.get(dataset_name)\n",
        "                matrix = np.array(matrix)\n",
        "            print(matrix.shape)\n",
        "            assert matrix.shape[0] == 248 , \"This memory data does not have 248 channels, but {} instead\".format(matrix.shape[0])\n",
        "            memory_matrix = np.column_stack((memory_matrix, matrix))\n",
        "            \n",
        "        if \"motor\" in files_to_load[i]:\n",
        "            with h5py.File(files_to_load[i],'r') as f:\n",
        "                dataset_name = get_dataset_name(files_to_load[i])\n",
        "                matrix = f.get(dataset_name)\n",
        "                matrix = np.array(matrix)\n",
        "            print(matrix.shape)\n",
        "            assert matrix.shape[0] == 248 , \"This motor data does not have 248 channels, but {} instead\".format(matrix.shape[0])\n",
        "            motor_matrix = np.column_stack((motor_matrix, matrix))\n",
        "\n",
        "        # del matrix\n",
        "        matrix = None\n",
        "\n",
        "    print(\"rest part\")\n",
        "    x_rest,y_rest = preprocess_data_type(rest_matrix)    \n",
        "    # del rest_matrix\n",
        "    rest_matrix = None\n",
        "    y_rest = y_rest*0\n",
        "    input1_rest,input2_rest,input3_rest,input4_rest,input5_rest = x_rest[0],x_rest[1],x_rest[2],x_rest[3],x_rest[4]\n",
        "    # del x_rest\n",
        "    x_rest = None\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"math part\")\n",
        "    x_math,y_math = preprocess_data_type(math_matrix)   \n",
        "    # del math_matrix   \n",
        "    input1_math,input2_math,input3_math,input4_math,input5_math = x_math[0],x_math[1],x_math[2],x_math[3],x_math[4]  \n",
        "    # del x_math \n",
        "    x_math = None\n",
        "    gc.collect()      \n",
        "\n",
        "    print(\"mem part\")\n",
        "    x_mem,y_mem = preprocess_data_type(memory_matrix)\n",
        "    # del memory_matrix\n",
        "    y_mem = y_mem * 2\n",
        "    input1_mem,input2_mem,input3_mem,input4_mem,input5_mem = x_mem[0],x_mem[1],x_mem[2],x_mem[3],x_mem[4] \n",
        "    # del x_mem\n",
        "    x_mem = None\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"motor part\")\n",
        "    x_motor,y_motor = preprocess_data_type(motor_matrix)\n",
        "    # del motor_matrix\n",
        "    y_motor = y_motor * 3\n",
        "    input1_motor,input2_motor,input3_motor,input4_motor,input5_motor = x_motor[0],x_motor[1],x_motor[2],x_motor[3],x_motor[4] \n",
        "    # del x_motor\n",
        "    x_motor = None\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    print(\"part 5 temp dicts\")\n",
        "    dict1 = {0:input1_rest,1:input1_math,2:input1_mem,3:input1_motor}\n",
        "    dict2 = {0:input2_rest,1:input2_math,2:input2_mem,3:input2_motor}\n",
        "    dict3 = {0:input3_rest,1:input3_math,2:input3_mem,3:input3_motor}\n",
        "    dict4 = {0:input4_rest,1:input4_math,2:input4_mem,3:input4_motor}\n",
        "    dict5 = {0:input5_rest,1:input5_math,2:input5_mem,3:input5_motor}\n",
        "\n",
        "    # del input1_rest\n",
        "    # del input2_rest\n",
        "    # del input3_rest\n",
        "    # del input4_rest\n",
        "    # del input5_rest\n",
        "\n",
        "    # del input1_math\n",
        "    # del input2_math\n",
        "    # del input3_math\n",
        "    # del input4_math\n",
        "    # del input5_math\n",
        "\n",
        "    # del input1_mem\n",
        "    # del input2_mem\n",
        "    # del input3_mem\n",
        "    # del input4_mem\n",
        "    # del input5_mem\n",
        "\n",
        "    # del input1_motor\n",
        "    # del input2_motor\n",
        "    # del input3_motor\n",
        "    # del input4_motor\n",
        "    # del input5_motor\n",
        "\n",
        "    input1_rest = None\n",
        "    input2_rest = None\n",
        "    input3_rest = None\n",
        "    input4_rest= None\n",
        "    input5_rest= None\n",
        "\n",
        "    input1_math= None\n",
        "    input2_math= None\n",
        "    input3_math= None\n",
        "    input4_math= None\n",
        "    input5_math= None\n",
        "\n",
        "    input1_mem= None\n",
        "    input2_mem= None\n",
        "    input3_mem= None\n",
        "    input4_mem= None\n",
        "    input5_mem= None\n",
        "\n",
        "    input1_motor= None\n",
        "    input2_motor= None\n",
        "    input3_motor= None\n",
        "    input4_motor= None\n",
        "    input5_motor= None\n",
        "\n",
        "\n",
        "    \n",
        "    input1 = np.random.rand(1,20,22)\n",
        "    input2 = np.random.rand(1,20,22)\n",
        "    input3 = np.random.rand(1,20,22)\n",
        "    input4 = np.random.rand(1,20,22)\n",
        "    input5 = np.random.rand(1,20,22)\n",
        "    \n",
        "    print(\"part concatenation\")\n",
        "    for i in range(number_classes):\n",
        "        if dict1[i].shape[0]>0:\n",
        "            input1=np.concatenate((input1,dict1[i]))\n",
        "            dict1[i] = None\n",
        "            \n",
        "        if dict2[i].shape[0]>0:\n",
        "            input2=np.concatenate((input2,dict2[i]))\n",
        "            dict2[i] = None\n",
        "            \n",
        "        if dict3[i].shape[0]>0:\n",
        "            input3=np.concatenate((input3,dict3[i]))\n",
        "            dict3[i] = None\n",
        "            \n",
        "        if dict4[i].shape[0]>0:\n",
        "            input4=np.concatenate((input4,dict4[i]))\n",
        "            dict4[i] = None\n",
        "            \n",
        "        if dict5[i].shape[0]>0:\n",
        "            input5=np.concatenate((input5,dict5[i]))\n",
        "            dict5[i] = None\n",
        "    \n",
        "    # del dict1\n",
        "    # del dict2\n",
        "    # del dict3\n",
        "    # del dict4\n",
        "    # del dict5\n",
        "    dict1 = None\n",
        "    dict2 = None\n",
        "    dict3 = None\n",
        "    dict4 = None\n",
        "    dict5 = None\n",
        "\n",
        "\n",
        "    #deleting first element of each lists                        \n",
        "    input1 = np.delete(input1,0,0)\n",
        "    input2 = np.delete(input2,0,0)\n",
        "    input3 = np.delete(input3,0,0)\n",
        "    input4 = np.delete(input4,0,0)\n",
        "    input5 = np.delete(input5,0,0)\n",
        "    \n",
        "    print(\"part reshaping\")\n",
        "    input1 = np.reshape(input1,(input1.shape[0],20,22,1))\n",
        "    input2 = np.reshape(input2,(input2.shape[0],20,22,1))\n",
        "    input3 = np.reshape(input3,(input3.shape[0],20,22,1))\n",
        "    input4 = np.reshape(input4,(input4.shape[0],20,22,1))\n",
        "    input5 = np.reshape(input5,(input5.shape[0],20,22,1))\n",
        "\n",
        "    gc.collect()\n",
        "    \n",
        "    dict_y = {0:y_rest,1:y_math,2:y_mem,3:y_motor}\n",
        "    \n",
        "    y = np.random.rand(1,1)\n",
        "    for i in range(number_classes):\n",
        "        if dict_y[i].shape[0]>0:\n",
        "            y = np.concatenate((y,dict_y[i]))\n",
        "\n",
        "    y = np.delete(y,0,0)\n",
        "\n",
        "    print(\"part shuffling\")\n",
        "\n",
        "    input1,input2,input3,input4,input5,y = shuffle(input1,input2,input3,input4,input5, y, random_state=42)\n",
        "    \n",
        "\n",
        "    \n",
        "    # input1 = normalize_matrix(input1)\n",
        "    # input2 = normalize_matrix(input2)\n",
        "    # input3 = normalize_matrix(input3)\n",
        "    # input4 = normalize_matrix(input4)\n",
        "    # input5 = normalize_matrix(input5)\n",
        "    print(\"part custom normalization\")\n",
        "    x_length = input1.shape[0]\n",
        "    for i in range(x_length):\n",
        "        temp1 = input1[i]\n",
        "        inside1 = temp1[:,:,0]\n",
        "        norm1 = normalize_matrix(inside1)\n",
        "        input1[i][:,:,0] = norm1\n",
        "        \n",
        "        temp2 = input2[i]\n",
        "        inside2 = temp2[:,:,0]\n",
        "        norm2 = normalize_matrix(inside2)\n",
        "        input2[i][:,:,0] = norm2\n",
        "        \n",
        "        temp3 = input3[i]\n",
        "        inside3 = temp3[:,:,0]\n",
        "        norm3 = normalize_matrix(inside3)\n",
        "        input3[i][:,:,0] = norm3\n",
        "        \n",
        "        temp4 = input4[i]\n",
        "        inside4 = temp4[:,:,0]\n",
        "        norm4 = normalize_matrix(inside4)\n",
        "        input4[i][:,:,0] = norm4\n",
        "        \n",
        "        temp5 = input5[i]\n",
        "        inside5 = temp5[:,:,0]\n",
        "        norm5 = normalize_matrix(inside5)\n",
        "        input5[i][:,:,0] = norm5\n",
        "\n",
        "    gc.collect()\n",
        "    del temp1\n",
        "    del temp2\n",
        "    del temp3\n",
        "    del temp4\n",
        "    del temp5\n",
        "\n",
        "    del inside1\n",
        "    del inside2\n",
        "    del inside3\n",
        "    del inside4\n",
        "    del inside5\n",
        "\n",
        "    del norm1\n",
        "    del norm2\n",
        "    del norm3\n",
        "    del norm4\n",
        "    del norm5\n",
        "\n",
        "    print(\"part final dict assignment\")\n",
        "    data_dict = {'input1' : input1,'input2' : input2,'input3' : input3, 'input4': input4,'input5':input5}\n",
        "    \n",
        "    input1 = None\n",
        "    input2 = None\n",
        "    input3 = None\n",
        "    input4 = None\n",
        "    input5 = None\n",
        "    # del input1\n",
        "    # del input2\n",
        "    # del input3\n",
        "    # del input4\n",
        "    # del input5\n",
        "\n",
        "    gc.collect()\n",
        "    \n",
        "    y = tf.keras.utils.to_categorical(y,number_classes)\n",
        "    return data_dict,y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi5Xu_MfiMy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import concatenate\n",
        "\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def _conv_layer(filters, kernel_size, strides=(1, 1), padding='same', name=None):\n",
        "    return Conv2D(filters, kernel_size, strides=strides, padding=padding,\n",
        "                  use_bias=True, kernel_initializer='he_normal', name=name)\n",
        "\n",
        "\n",
        "def _normalize_depth_vars(depth_k, depth_v, filters):\n",
        "    \"\"\"\n",
        "    Accepts depth_k and depth_v as either floats or integers\n",
        "    and normalizes them to integers.\n",
        "    Args:\n",
        "        depth_k: float or int.\n",
        "        depth_v: float or int.\n",
        "        filters: number of output filters.\n",
        "    Returns:\n",
        "        depth_k, depth_v as integers.\n",
        "    \"\"\"\n",
        "\n",
        "    if type(depth_k) == float:\n",
        "        depth_k = int(filters * depth_k)\n",
        "    else:\n",
        "        depth_k = int(depth_k)\n",
        "\n",
        "    if type(depth_v) == float:\n",
        "        depth_v = int(filters * depth_v)\n",
        "    else:\n",
        "        depth_v = int(depth_v)\n",
        "\n",
        "    return depth_k, depth_v\n",
        "\n",
        "\n",
        "class AttentionAugmentation2D(Layer):\n",
        "\n",
        "    def __init__(self, depth_k, depth_v, num_heads, relative=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Applies attention augmentation on a convolutional layer\n",
        "        output.\n",
        "        Args:\n",
        "            depth_k: float or int. Number of filters for k.\n",
        "            Computes the number of filters for `v`.\n",
        "            If passed as float, computed as `filters * depth_k`.\n",
        "        depth_v: float or int. Number of filters for v.\n",
        "            Computes the number of filters for `k`.\n",
        "            If passed as float, computed as `filters * depth_v`.\n",
        "        num_heads: int. Number of attention heads.\n",
        "            Must be set such that `depth_k // num_heads` is > 0.\n",
        "        relative: bool, whether to use relative encodings.\n",
        "        Raises:\n",
        "            ValueError: if depth_v or depth_k is not divisible by\n",
        "                num_heads.\n",
        "        Returns:\n",
        "            Output tensor of shape\n",
        "            -   [Batch, Height, Width, Depth_V] if\n",
        "                channels_last data format.\n",
        "            -   [Batch, Depth_V, Height, Width] if\n",
        "                channels_first data format.\n",
        "        \"\"\"\n",
        "        super(AttentionAugmentation2D, self).__init__(**kwargs)\n",
        "\n",
        "        if depth_k % num_heads != 0:\n",
        "            raise ValueError('`depth_k` (%d) is not divisible by `num_heads` (%d)' % (\n",
        "                depth_k, num_heads))\n",
        "\n",
        "        if depth_v % num_heads != 0:\n",
        "            raise ValueError('`depth_v` (%d) is not divisible by `num_heads` (%d)' % (\n",
        "                depth_v, num_heads))\n",
        "\n",
        "        if depth_k // num_heads < 1.:\n",
        "            raise ValueError('depth_k / num_heads cannot be less than 1 ! '\n",
        "                             'Given depth_k = %d, num_heads = %d' % (\n",
        "                             depth_k, num_heads))\n",
        "\n",
        "        if depth_v // num_heads < 1.:\n",
        "            raise ValueError('depth_v / num_heads cannot be less than 1 ! '\n",
        "                             'Given depth_v = %d, num_heads = %d' % (\n",
        "                                 depth_v, num_heads))\n",
        "\n",
        "        self.depth_k = depth_k\n",
        "        self.depth_v = depth_v\n",
        "        self.num_heads = num_heads\n",
        "        self.relative = relative\n",
        "\n",
        "        self.axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self._shape = input_shape\n",
        "\n",
        "        # normalize the format of depth_v and depth_k\n",
        "        self.depth_k, self.depth_v = _normalize_depth_vars(self.depth_k, self.depth_v,\n",
        "                                                           input_shape)\n",
        "\n",
        "        if self.axis == 1:\n",
        "            _, channels, height, width = input_shape\n",
        "        else:\n",
        "            _, height, width, channels = input_shape\n",
        "\n",
        "        if self.relative:\n",
        "            dk_per_head = self.depth_k // self.num_heads\n",
        "\n",
        "            if dk_per_head == 0:\n",
        "                print('dk per head', dk_per_head)\n",
        "\n",
        "            self.key_relative_w = self.add_weight('key_rel_w',\n",
        "                                                  shape=[2 * width - 1, dk_per_head],\n",
        "                                                  initializer=initializers.RandomNormal(\n",
        "                                                      stddev=dk_per_head ** -0.5))\n",
        "\n",
        "            self.key_relative_h = self.add_weight('key_rel_h',\n",
        "                                                  shape=[2 * height - 1, dk_per_head],\n",
        "                                                  initializer=initializers.RandomNormal(\n",
        "                                                      stddev=dk_per_head ** -0.5))\n",
        "\n",
        "        else:\n",
        "            self.key_relative_w = None\n",
        "            self.key_relative_h = None\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if self.axis == 1:\n",
        "            # If channels first, force it to be channels last for these ops\n",
        "            inputs = K.permute_dimensions(inputs, [0, 2, 3, 1])\n",
        "\n",
        "        q, k, v = tf.split(inputs, [self.depth_k, self.depth_k, self.depth_v], axis=-1)\n",
        "\n",
        "        q = self.split_heads_2d(q)\n",
        "        k = self.split_heads_2d(k)\n",
        "        v = self.split_heads_2d(v)\n",
        "\n",
        "        # scale query\n",
        "        depth_k_heads = self.depth_k / self.num_heads\n",
        "        q *= (depth_k_heads ** -0.5)\n",
        "\n",
        "        # [Batch, num_heads, height * width, depth_k or depth_v] if axis == -1\n",
        "        qk_shape = [self._batch, self.num_heads, self._height * self._width, self.depth_k // self.num_heads]\n",
        "        v_shape = [self._batch, self.num_heads, self._height * self._width, self.depth_v // self.num_heads]\n",
        "        flat_q = K.reshape(q, K.stack(qk_shape))\n",
        "        flat_k = K.reshape(k, K.stack(qk_shape))\n",
        "        flat_v = K.reshape(v, K.stack(v_shape))\n",
        "\n",
        "        # [Batch, num_heads, HW, HW]\n",
        "        logits = tf.matmul(flat_q, flat_k, transpose_b=True)\n",
        "\n",
        "        # Apply relative encodings\n",
        "        if self.relative:\n",
        "            h_rel_logits, w_rel_logits = self.relative_logits(q)\n",
        "            logits += h_rel_logits\n",
        "            logits += w_rel_logits\n",
        "\n",
        "        weights = K.softmax(logits, axis=-1)\n",
        "        attn_out = tf.matmul(weights, flat_v)\n",
        "\n",
        "        attn_out_shape = [self._batch, self.num_heads, self._height, self._width, self.depth_v // self.num_heads]\n",
        "        attn_out_shape = K.stack(attn_out_shape)\n",
        "        attn_out = K.reshape(attn_out, attn_out_shape)\n",
        "        attn_out = self.combine_heads_2d(attn_out)\n",
        "        # [batch, height, width, depth_v]\n",
        "\n",
        "        if self.axis == 1:\n",
        "            # return to [batch, depth_v, height, width] for channels first\n",
        "            attn_out = K.permute_dimensions(attn_out, [0, 3, 1, 2])\n",
        "\n",
        "        return attn_out\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[self.axis] = self.depth_v\n",
        "        return tuple(output_shape)\n",
        "\n",
        "    def split_heads_2d(self, ip):\n",
        "        tensor_shape = K.shape(ip)\n",
        "\n",
        "        # batch, height, width, channels for axis = -1\n",
        "        tensor_shape = [tensor_shape[i] for i in range(len(self._shape))]\n",
        "\n",
        "        batch = tensor_shape[0]\n",
        "        height = tensor_shape[1]\n",
        "        width = tensor_shape[2]\n",
        "        channels = tensor_shape[3]\n",
        "\n",
        "        # Save the spatial tensor dimensions\n",
        "        self._batch = batch\n",
        "        self._height = height\n",
        "        self._width = width\n",
        "\n",
        "        ret_shape = K.stack([batch, height, width,  self.num_heads, channels // self.num_heads])\n",
        "        split = K.reshape(ip, ret_shape)\n",
        "        transpose_axes = (0, 3, 1, 2, 4)\n",
        "        split = K.permute_dimensions(split, transpose_axes)\n",
        "\n",
        "        return split\n",
        "\n",
        "    def relative_logits(self, q):\n",
        "        shape = K.shape(q)\n",
        "        # [batch, num_heads, H, W, depth_v]\n",
        "        shape = [shape[i] for i in range(5)]\n",
        "\n",
        "        height = shape[2]\n",
        "        width = shape[3]\n",
        "\n",
        "        rel_logits_w = self.relative_logits_1d(q, self.key_relative_w, height, width,\n",
        "                                               transpose_mask=[0, 1, 2, 4, 3, 5])\n",
        "\n",
        "        rel_logits_h = self.relative_logits_1d(\n",
        "            K.permute_dimensions(q, [0, 1, 3, 2, 4]),\n",
        "            self.key_relative_h, width, height,\n",
        "            transpose_mask=[0, 1, 4, 2, 5, 3])\n",
        "\n",
        "        return rel_logits_h, rel_logits_w\n",
        "\n",
        "    def relative_logits_1d(self, q, rel_k, H, W, transpose_mask):\n",
        "        rel_logits = tf.einsum('bhxyd,md->bhxym', q, rel_k)\n",
        "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads * H, W, 2 * W - 1])\n",
        "        rel_logits = self.rel_to_abs(rel_logits)\n",
        "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads, H, W, W])\n",
        "        rel_logits = K.expand_dims(rel_logits, axis=3)\n",
        "        rel_logits = K.tile(rel_logits, [1, 1, 1, H, 1, 1])\n",
        "        rel_logits = K.permute_dimensions(rel_logits, transpose_mask)\n",
        "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads, H * W, H * W])\n",
        "        return rel_logits\n",
        "\n",
        "    def rel_to_abs(self, x):\n",
        "        shape = K.shape(x)\n",
        "        shape = [shape[i] for i in range(3)]\n",
        "        B, Nh, L, = shape\n",
        "        col_pad = K.zeros(K.stack([B, Nh, L, 1]))\n",
        "        x = K.concatenate([x, col_pad], axis=3)\n",
        "        flat_x = K.reshape(x, [B, Nh, L * 2 * L])\n",
        "        flat_pad = K.zeros(K.stack([B, Nh, L - 1]))\n",
        "        flat_x_padded = K.concatenate([flat_x, flat_pad], axis=2)\n",
        "        final_x = K.reshape(flat_x_padded, [B, Nh, L + 1, 2 * L - 1])\n",
        "        final_x = final_x[:, :, :L, L - 1:]\n",
        "        return final_x\n",
        "\n",
        "    def combine_heads_2d(self, inputs):\n",
        "        # [batch, num_heads, height, width, depth_v // num_heads]\n",
        "        transposed = K.permute_dimensions(inputs, [0, 2, 3, 1, 4])\n",
        "        # [batch, height, width, num_heads, depth_v // num_heads]\n",
        "        shape = K.shape(transposed)\n",
        "        shape = [shape[i] for i in range(5)]\n",
        "\n",
        "        a, b = shape[-2:]\n",
        "        ret_shape = K.stack(shape[:-2] + [a * b])\n",
        "        # [batch, height, width, depth_v]\n",
        "        return K.reshape(transposed, ret_shape)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'depth_k': self.depth_k,\n",
        "            'depth_v': self.depth_v,\n",
        "            'num_heads': self.num_heads,\n",
        "            'relative': self.relative,\n",
        "        }\n",
        "        base_config = super(AttentionAugmentation2D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def augmented_conv2d(ip, filters, kernel_size=(7, 7), strides=(1, 1),\n",
        "                     depth_k=0.2, depth_v=0.2, num_heads=8, relative_encodings=True):\n",
        "    \"\"\"\n",
        "    Builds an Attention Augmented Convolution block.\n",
        "    Args:\n",
        "        ip: keras tensor.\n",
        "        filters: number of output filters.\n",
        "        kernel_size: convolution kernel size.\n",
        "        strides: strides of the convolution.\n",
        "        depth_k: float or int. Number of filters for k.\n",
        "            Computes the number of filters for `v`.\n",
        "            If passed as float, computed as `filters * depth_k`.\n",
        "        depth_v: float or int. Number of filters for v.\n",
        "            Computes the number of filters for `k`.\n",
        "            If passed as float, computed as `filters * depth_v`.\n",
        "        num_heads: int. Number of attention heads.\n",
        "            Must be set such that `depth_k // num_heads` is > 0.\n",
        "        relative_encodings: bool. Whether to use relative\n",
        "            encodings or not.\n",
        "    Returns:\n",
        "        a keras tensor.\n",
        "    \"\"\"\n",
        "    # input_shape = K.int_shape(ip)\n",
        "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    depth_k, depth_v = _normalize_depth_vars(depth_k, depth_v, filters)\n",
        "\n",
        "    conv_out = _conv_layer(filters - depth_v, kernel_size, strides)(ip)\n",
        "\n",
        "    # Augmented Attention Block\n",
        "    qkv_conv = _conv_layer(2 * depth_k + depth_v, (1, 1), strides)(ip)\n",
        "    attn_out = AttentionAugmentation2D(depth_k, depth_v, num_heads, relative_encodings)(qkv_conv)\n",
        "    attn_out = _conv_layer(depth_v, kernel_size=(1, 1))(attn_out)\n",
        "\n",
        "    output = concatenate([conv_out, attn_out], axis=channel_axis)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qosjueIJiU4y",
        "colab_type": "code",
        "outputId": "de4f5895-20c9-4244-93d7-47f677994eaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "ip = Input(shape=(32, 32, 3))\n",
        "x = augmented_conv2d(ip, filters=20, kernel_size=(3, 3),\n",
        "                         depth_k=0.2, depth_v=0.2,  # dk/v (0.2) * f_out (20) = 4\n",
        "                         num_heads=4, relative_encodings=True)\n",
        "\n",
        "model = Model(ip, x)\n",
        "model.summary()\n",
        "\n",
        "# Check if attention builds properly\n",
        "x = tf.zeros((1, 32, 32, 3))\n",
        "y = model(x)\n",
        "print(\"Attention Augmented Conv out shape : \", y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 65\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got Dimension(63)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-0897adba6d83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m x = augmented_conv2d(ip, filters=20, kernel_size=(3, 3),\n\u001b[1;32m      6\u001b[0m                          \u001b[0mdepth_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_v\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# dk/v (0.2) * f_out (20) = 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                          num_heads=4, relative_encodings=True)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-9fd69813c78c>\u001b[0m in \u001b[0;36maugmented_conv2d\u001b[0;34m(ip, filters, kernel_size, strides, depth_k, depth_v, num_heads, relative_encodings)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;31m# Augmented Attention Block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mqkv_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_conv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdepth_k\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdepth_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0mattn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionAugmentation2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_encodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqkv_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0mattn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_conv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m           \u001b[0;31m# Wrapping `call` function in autograph to allow for dynamic control\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1879\u001b[0m       \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m     \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m     \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-9fd69813c78c>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    113\u001b[0m                                                   \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdk_per_head\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                                                   initializer=initializers.RandomNormal(\n\u001b[0;32m--> 115\u001b[0;31m                                                       stddev=dk_per_head ** -0.5))\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self.key_relative_h = self.add_weight('key_rel_h',\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    385\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       shape=variable_shape if variable_shape.rank else None)\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m   def _variable_v2_call(cls,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m                         shape=None):\n\u001b[1;32m    197\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2494\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2495\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2496\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2497\u001b[0m     return variables.RefVariable(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    458\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m           shape=shape)\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    602\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m--> 604\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m    606\u001b[0m           \u001b[0;31m# Don't use `shape or initial_value.shape` since TensorShape has\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m           (type(init_ops.Initializer), type(init_ops_v2.Initializer))):\n\u001b[1;32m    134\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m       \u001b[0minit_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m       \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muse_resource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype, partition_info)\u001b[0m\n\u001b[1;32m    321\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     return random_ops.random_normal(\n\u001b[0;32m--> 323\u001b[0;31m         shape, self.mean, self.stddev, dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_normal\u001b[0;34m(shape, mean, stddev, dtype, seed, name)\u001b[0m\n\u001b[1;32m     72\u001b[0m   \"\"\"\n\u001b[1;32m     73\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_normal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mshape_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ShapeTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mmean_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mstddev_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stddev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36m_ShapeTensor\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[1;32m   1085\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[1;32m   1086\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[0;32m-> 1087\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1143\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    303\u001b[0m                                          as_ref=False):\n\u001b[1;32m    304\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    244\u001b[0m   \"\"\"\n\u001b[1;32m    245\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 246\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    282\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m    283\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    285\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m   const_tensor = g.create_op(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    560\u001b[0m       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\u001b[1;32m    561\u001b[0m                       \u001b[0;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[1;32m    563\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Failed to convert object of type <class 'list'> to Tensor. Contents: [Dimension(63), 1]. Consider casting elements to a supported type."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA7sOa4JifWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, LSTM, Dropout, Bidirectional\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"# Cascade model\"\"\"\n",
        "\n",
        "class CascadeMulti:\n",
        "    def __init__(self, window_size, cnn_activation, hidden_activation, model_activation, pool_size,\n",
        "                 number_conv2D_filters, kernel_shape, number_lstm_cells, number_nodes_hidden, loss,\n",
        "                 optimizer, using_gpu):\n",
        "        self.window_size = window_size\n",
        "        self.cnn_activation = cnn_activation\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.model_activation = model_activation\n",
        "        self.pool_size = pool_size\n",
        "        self.number_conv2D_filters = number_conv2D_filters\n",
        "        self.kernel_shape = kernel_shape\n",
        "        self.number_lstm_cells = number_lstm_cells\n",
        "        self.number_nodes_hidden = number_nodes_hidden\n",
        "        self.mesh_rows = 20\n",
        "        self.mesh_columns = 22\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.using_gpu = using_gpu\n",
        "        self.number_classes = 4\n",
        "        self.model = self.cascade_model()\n",
        "        \n",
        "        \n",
        "    def cascade_model(self):\n",
        "      inputs = []\n",
        "      convs = []\n",
        "      for i in range(self.window_size):\n",
        "          input_layer = Input(shape=(self.mesh_rows, self.mesh_columns,1), name = \"input\"+str(i+1))\n",
        "          inputs.append(input_layer)\n",
        "      \n",
        "      if self.using_gpu:\n",
        "          for i in range(self.window_size):\n",
        "              conv1 = Conv2D(self.number_conv2D_filters, self.kernel_shape, padding = 'same',activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(inputs[i])#change padding\n",
        "              conv1 = Dropout(0.3)(conv1)\n",
        "              conv2 = Conv2D(1*self.number_conv2D_filters, self.kernel_shape, padding = 'same', activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(conv1)# modify shape and kernel\n",
        "              conv2 = Dropout(0.3)(conv2)\n",
        "              conv1_3 = Conv2D(self.number_conv2D_filters, kernel_size = (1,1),activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(conv1)#change padding\n",
        "              print(conv2.shape, conv1_3.shape)\n",
        "              conv3 = Conv2D(self.number_conv2D_filters, self.kernel_shape, padding = 'same', activation=self.cnn_activation, \n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(tf.concat([conv2, conv1_3], 3))\n",
        "              #conv3 = Conv2D(1*self.number_conv2D_filters, self.kernel_shape, padding = 'same', activation=self.cnn_activation,\n",
        "              #               input_shape=(self.mesh_rows, self.mesh_columns,1))(conv2)\n",
        "              conv3 = Dropout(0.3)(conv3)\n",
        "              conv1_4 = Conv2D(self.number_conv2D_filters, kernel_size = (1,1),activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(conv1)\n",
        "              conv2_4 = Conv2D(self.number_conv2D_filters, kernel_size = (1,1),activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(conv2)\n",
        "              conv4 = Conv2D(1*self.number_conv2D_filters, self.kernel_shape, padding = 'same', activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(tf.concat([conv3, conv1_4, conv2_4], 3))\n",
        "              conv4 = Dropout(0.3)(conv4)\n",
        "              conv1_5 = Conv2D(self.number_conv2D_filters, kernel_size = (1,1),activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(conv1)\n",
        "              conv2_5 = Conv2D(self.number_conv2D_filters, kernel_size = (1,1),activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(conv2)\n",
        "              conv3_5 = Conv2D(self.number_conv2D_filters, kernel_size = (1,1),activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(conv3)\n",
        "              conv5 = Conv2D(2*self.number_conv2D_filters, self.kernel_shape, padding = 'same', activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(tf.concat([conv4, conv1_5, conv2_5, conv3_5], 3))\n",
        "              conv5 = Dropout(0.3)(conv5)\n",
        "              dense1 = Dense(self.number_nodes_hidden, activation=self.hidden_activation)(Flatten()(conv5))\n",
        "              dense1 = Dropout(0.3)(dense1)\n",
        "              convs.append(dense1)\n",
        "      else:\n",
        "          for i in range(self.window_size):\n",
        "              conv1 = Conv2D(self.number_conv2D_filters, self.kernel_shape, padding = 'same',activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(inputs[i])#change padding\n",
        "              conv1 = Dropout(0.3)(conv1)\n",
        "              conv2 = Conv2D(1*self.number_conv2D_filters, self.kernel_shape, padding = 'same', activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(conv1)# modify shape and kernel\n",
        "              conv2 = Dropout(0.3)(conv2)\n",
        "              conv1_3 = Conv2D(self.number_conv2D_filters, kernel_size = (1,1),activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(conv1)#change padding\n",
        "              print(conv2.shape, conv1_3.shape)\n",
        "              conv3 = Conv2D(self.number_conv2D_filters, self.kernel_shape, padding = 'same', activation=self.cnn_activation, \n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(tf.concat([conv2, conv1_3], 3))\n",
        "              #conv3 = Conv2D(1*self.number_conv2D_filters, self.kernel_shape, padding = 'same', activation=self.cnn_activation,\n",
        "              #               input_shape=(self.mesh_rows, self.mesh_columns,1))(conv2)\n",
        "              conv3 = Dropout(0.3)(conv3)\n",
        "              conv1_4 = Conv2D(self.number_conv2D_filters, kernel_size = (1,1),activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(conv1)\n",
        "              conv2_4 = Conv2D(self.number_conv2D_filters, kernel_size = (1,1),activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(conv2)\n",
        "              conv4 = Conv2D(1*self.number_conv2D_filters, self.kernel_shape, padding = 'same', activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(tf.concat([conv3, conv1_4, conv2_4], 3))\n",
        "              conv4 = Dropout(0.3)(conv4)\n",
        "              conv1_5 = Conv2D(self.number_conv2D_filters, kernel_size = (1,1),activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(conv1)\n",
        "              conv2_5 = Conv2D(self.number_conv2D_filters, kernel_size = (1,1),activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(conv2)\n",
        "              conv3_5 = Conv2D(self.number_conv2D_filters, kernel_size = (1,1),activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(conv3)\n",
        "              conv5 = Conv2D(2*self.number_conv2D_filters, self.kernel_shape, padding = 'same', activation=self.cnn_activation,\n",
        "                             input_shape=(self.mesh_rows, self.mesh_columns,1))(tf.concat([conv4, conv1_5, conv2_5, conv3_5], 3))\n",
        "              conv5 = Dropout(0.3)(conv5)\n",
        "              dense1 = Dense(self.number_nodes_hidden, activation=self.hidden_activation)(Flatten()(conv5))\n",
        "              dense1 = Dropout(0.3)(dense1)\n",
        "              convs.append(dense1)\n",
        "    \n",
        "      merge = concatenate(convs)\n",
        "      merge = tf.expand_dims(merge,axis=1)\n",
        "      #bidirectional = (Bidirectional(LSTM(self.number_lstm_cells)))\n",
        "      lstm = LSTM(self.number_lstm_cells, return_sequences=False)(merge)\n",
        "      hidden1 = Dense(self.number_nodes_hidden, activation=self.hidden_activation)(lstm)\n",
        "      output = Dense(self.number_classes, activation=self.model_activation)(hidden1)\n",
        "      model = Model(inputs=inputs, outputs=output)\n",
        "      return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdjTkY4hHIeK",
        "colab_type": "code",
        "outputId": "b69820d5-996d-4a5e-c239-02d085e0fce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras import optimizers\n",
        "#import ModelCascadeMulti as cascade\n",
        "from CallbackCascadeMulti import PrintingCallback\n",
        "from keras.utils import Sequence\n",
        "from keras.callbacks import TensorBoard\n",
        "import gc\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   print('GPU device not found')\n",
        "#   using_gpu = False\n",
        "# else:\n",
        "#     using_gpu = True\n",
        "\n",
        "\n",
        "window_size = 5\n",
        "cnn_activation =\"relu\"\n",
        "hidden_activation=\"relu\"\n",
        "model_activation=\"softmax\"\n",
        "pool_size = (1,1)\n",
        "number_conv2D_filters = 10\n",
        "kernel_shape = (7)\n",
        "number_lstm_cells = 10\n",
        "number_nodes_hidden = 40\n",
        "loss = \"categorical_crossentropy\"\n",
        "optimizer = \"sgd\"\n",
        "\n",
        "# print(train_files_dirs)\n",
        "# print(validate_files_dirs)\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "callback_list = []\n",
        "\n",
        "gc.collect()\n",
        "x_train,y_train = load_data(train_files_dirs)\n",
        "print(\"done loading training !\")\n",
        "print(\"byte size of training data: {}\".format(sys.getsizeof(x_train)))\n",
        "x_val,y_val = load_data(validate_files_dirs)\n",
        "print(\"done loading validation !\")\n",
        "print(\"byte size of validation data: {}\".format(sys.getsizeof(x_val)))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of files to load: 64\n",
            "rest_108323\n",
            "(248, 35624)\n",
            "rest_108323\n",
            "(248, 35624)\n",
            "rest_108323\n",
            "(248, 35624)\n",
            "rest_108323\n",
            "(248, 35624)\n",
            "rest_108323\n",
            "(248, 35624)\n",
            "rest_108323\n",
            "(248, 35624)\n",
            "rest_108323\n",
            "(248, 35624)\n",
            "rest_108323\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "rest_105923\n",
            "(248, 35624)\n",
            "rest_105923\n",
            "(248, 35624)\n",
            "rest_105923\n",
            "(248, 35624)\n",
            "rest_105923\n",
            "(248, 35624)\n",
            "rest_105923\n",
            "(248, 35624)\n",
            "rest_105923\n",
            "(248, 35624)\n",
            "rest_105923\n",
            "(248, 35624)\n",
            "rest_105923\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "rest part\n",
            "math part\n",
            "mem part\n",
            "motor part\n",
            "part 5 temp dicts\n",
            "part concatenation\n",
            "part reshaping\n",
            "part shuffling\n",
            "part custom normalization\n",
            "part final dict assignment\n",
            "done loading training !\n",
            "byte size of training data: 240\n",
            "number of files to load: 16\n",
            "rest_108323\n",
            "(248, 35624)\n",
            "rest_108323\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "rest_105923\n",
            "(248, 35624)\n",
            "rest_105923\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "rest part\n",
            "math part\n",
            "mem part\n",
            "motor part\n",
            "part 5 temp dicts\n",
            "part concatenation\n",
            "part reshaping\n",
            "part shuffling\n",
            "part custom normalization\n",
            "part final dict assignment\n",
            "done loading validation !\n",
            "byte size of validation data: 240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA6kA26FWrLZ",
        "colab_type": "code",
        "outputId": "46ac6762-285a-4f0e-e3c2-0701d4dad460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "batch_size = 128\n",
        "callback_list = []\n",
        "\n",
        "gc.collect()\n",
        "x_train,y_train = load_data(train_files_dirs)\n",
        "print(\"done loading training !\")\n",
        "print(\"byte size of training data: {}\".format(sys.getsizeof(x_train)))\n",
        "x_val,y_val = load_data(validate_files_dirs)\n",
        "print(\"done loading validation !\")\n",
        "print(\"byte size of validation data: {}\".format(sys.getsizeof(x_val)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of files to load: 40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-735204b3a2c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files_dirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done loading training !\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"byte size of training data: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f5edcc95db4a>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(file_dirs)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"memory\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_to_load\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_to_load\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_to_load\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'Data/train/task_working_memory_108323_1.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvSGzzgk6iJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## This cell serves as replacement for the Callback class, still dirty for now but will be cleaned later\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import matplotlib as mpl\n",
        "\n",
        "def plot_epochs_info(experiment_number):\n",
        "    filename = \"Experiments/Cascade/Experiment\"+str(experiment_number)+\"/info_epochs_model\"+str(experiment_number)+\".txt\"\n",
        "    train_accuracies = []\n",
        "    train_losses = []\n",
        "    valid_accuracies = []\n",
        "    valid_losses = []\n",
        "    try:\n",
        "        with open(filename, \"r\") as file:\n",
        "            lines = file.readlines()\n",
        "            number_epochs = len(lines)\n",
        "            x_values = np.arange(start = 1, stop = number_epochs + 1)\n",
        "            for line in lines:\n",
        "                temp_parts = line.split(',')\n",
        "                \n",
        "                train_accuracy_part = temp_parts[1]\n",
        "                train_accuracies.append(float(train_accuracy_part.split(':')[1]))\n",
        "                \n",
        "                train_loss_part = temp_parts[2]\n",
        "                train_losses.append(float(train_loss_part.split(':')[1]))\n",
        "                \n",
        "                valid_accuracy_part = temp_parts[3]\n",
        "                valid_accuracies.append(float(valid_accuracy_part.split(':')[1]))\n",
        "                \n",
        "                valid_loss_part = temp_parts[4]\n",
        "                valid_losses.append(float(valid_loss_part.split(':')[1]))\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(\"Problem while reading the file {}\".format(filename))\n",
        "        print(\"Exception message : {}\".format(e))\n",
        "    # plt.figure(figsize=(10,10))\n",
        "    mpl.style.use('seaborn')\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(10,10))\n",
        "    # plt.figure()\n",
        "    ax1.plot(x_values,train_accuracies,label=\"Training Accuracy\",color=\"#4C72B0\")\n",
        "    ax1.plot(x_values,valid_accuracies,label=\"Validation Accuracy\", color='#55A868')\n",
        "    ax1.legend(loc=\"upper left\",fontsize='small')\n",
        "    \n",
        "    ax2.plot(x_values,train_losses,label=\"Training Loss\", color = \"#DD8452\" )\n",
        "    ax2.plot(x_values,valid_losses,label=\"Validation Loss\", color=\"#C44E52\")\n",
        "    ax2.legend(loc=\"upper right\",fontsize='small')\n",
        "    \n",
        "    ax1.set_title(\"Accuracy during Training and Validation\")\n",
        "    ax2.set_title(\"Loss during Training and Validation\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    ax1.set(ylabel =\"Accuracy\")\n",
        "    ax2.set(ylabel=\"Loss\")\n",
        "    output_filename = \"Experiments/Cascade/Experiment\"+str(experiment_number)+\"/plot_model\"+str(experiment_number)+\".jpg\"\n",
        "    fig.savefig(output_filename,dpi=100)\n",
        "    plt.show()\n",
        "\n",
        "def get_experiment_number():\n",
        "    experiments_folders_list = os.listdir(path='Experiments/Cascade')\n",
        "    if(len(experiments_folders_list) == 0): #empty folder\n",
        "        return 1\n",
        "    else:  \n",
        "        temp_numbers=[]\n",
        "        for folder in experiments_folders_list:\n",
        "            number = re.findall(r'\\d+', folder)\n",
        "            if(len(number)>0):\n",
        "                temp_numbers.append(int(number[0]))\n",
        "        return max(temp_numbers) + 1\n",
        "\n",
        "def append_to_epochs_file(experiment_number, epoch_number, training_accuracy, training_loss, validation_accuracy, validation_loss):\n",
        "    filename = \"Experiments/Cascade/Experiment\"+str(experiment_number)+\"/info_epochs_model\"+str(experiment_number)+\".txt\"\n",
        "    with open(filename, \"a+\") as file:\n",
        "        file.write(\"Epoch {0},training_acuracy:{1:.2f},trainig_loss:{2:.2f},validation_accuracy:{3:.2f},validation_loss:{4:.2f}\\n\".format(epoch_number,training_accuracy,training_loss,validation_accuracy,validation_loss))\n",
        "\n",
        "def create_cascade_folder():\n",
        "    path_cascade = \"Experiments/Cascade\"\n",
        "    if(not os.path.isdir(path_cascade)):\n",
        "        try:\n",
        "            os.mkdir(path_cascade)\n",
        "        except Exception as e:\n",
        "            print (\"Creation of the main cascade experiment directory failed\")\n",
        "            print(\"Exception error: \",str(e))     \n",
        "        \n",
        "def create_experiment_folder(experiment_number):\n",
        "    try:\n",
        "        path_new_experiment = \"Experiments/Cascade/Experiment\" + str(experiment_number)\n",
        "        check_point_path = path_new_experiment+\"/checkpoints\"\n",
        "        os.mkdir(path_new_experiment)\n",
        "        os.mkdir(check_point_path)\n",
        "    except Exception as e:\n",
        "        print (\"Creation of the directory {} or {} failed\".format(path_new_experiment,check_point_path))\n",
        "        print(\"Exception error: \",str(e))  \n",
        "\n",
        "\n",
        "def create_main_experiment_folder():\n",
        "    if(not os.path.isdir(\"Experiments\")):\n",
        "        try:\n",
        "            os.mkdir(\"Experiments\")\n",
        "        except Exception as e:\n",
        "            print (\"Creation of the main experiment directory failed\")\n",
        "            print(\"Exception error: \",str(e))\n",
        "def append_to_summary_file():\n",
        "    print(1)\n",
        "\n",
        "def create_info_epochs_file(experiment_number):\n",
        "        filename = \"Experiments/Cascade/Experiment\"+str(experiment_number)+\"/info_epochs_model\"+str(experiment_number)+\".txt\"\n",
        "        with open(filename, \"w\") as file:\n",
        "            file.write(\"\")\n",
        "\n",
        "def on_train_begin(using_gpu):\n",
        "    create_main_experiment_folder()\n",
        "    create_cascade_folder()\n",
        "    experiment_number = get_experiment_number()\n",
        "    create_experiment_folder(experiment_number)\n",
        "    print()\n",
        "    print()\n",
        "    if(using_gpu):\n",
        "        print(\"-\"*7 +\" Beginning of Experiment {} of the Cascade model using GPU \".format(experiment_number) + \"-\"*7)\n",
        "    else:\n",
        "        print(\"-\"*7 +\" Beginning of Experiment {} of the Cascade model without using GPU\".format(experiment_number) + \"-\"*7)            \n",
        "    print()\n",
        "    print()\n",
        "    # self.create_experiment_folder(self.experiment_number)\n",
        "    # create_summary_file(experiment_number)\n",
        "    # append_to_summary_file(cascade_model_object, experiment_number)\n",
        "    create_info_epochs_file(experiment_number)\n",
        "    return experiment_number\n",
        "\n",
        "def on_train_end(experiment_number):\n",
        "    print()\n",
        "    print()\n",
        "    print(\"-\"*7 +\" End of Experiment {} \".format(experiment_number) + \"-\"*7)\n",
        "    print()\n",
        "    print()\n",
        "    print(\"-\"*7 +\" Plotting and saving the epochs training/validation accuracy/loss \" + \"-\"*7)\n",
        "    plot_epochs_info(experiment_number)\n",
        "\n",
        "def on_epoch_end(epoch, accuracy, loss, val_accuracy, val_loss,experiment_number,model):\n",
        "    try:\n",
        "        append_to_epochs_file(experiment_number,epoch, accuracy, loss, val_accuracy, val_loss)\n",
        "        model_checkpoint(experiment_number, model, val_accuracy, epoch)\n",
        "    except Exception as e:\n",
        "        print(\"Failed to append in epoch file or to save the weights ...\")\n",
        "        print(\"Exception error: \",str(e))\n",
        "\n",
        "def model_checkpoint(experiment_number,model,validation_accuracy,epoch):\n",
        "    exp_path = \"Experiments/Cascade/Experiment\" + str(experiment_number)\n",
        "    check_point_path = exp_path+\"/checkpoints\" + '/checkpoint-epoch_{:03d}-val_acc_{:.3f}.hdf5'.format(epoch,validation_accuracy)\n",
        "    model.save_weights(check_point_path)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDMR4ojju_7E",
        "colab_type": "code",
        "outputId": "36bfefb3-8385-43c9-84e8-afa846bbbaba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "epochs = 20\n",
        "kernel_shape = (5)\n",
        "optimizer = \"adam\"\n",
        "using_gpu = False\n",
        "model_object = CascadeMulti(window_size,cnn_activation, hidden_activation, model_activation, pool_size,\n",
        "                            number_conv2D_filters, kernel_shape, number_lstm_cells, number_nodes_hidden, \n",
        "                            loss, optimizer,using_gpu)\n",
        "\n",
        "\n",
        "main_callback = PrintingCallback(epochs, batch_size, model_object, using_gpu)\n",
        "checkpoint_callback = main_callback.checkpoint_callback\n",
        "callback_list.append(main_callback)\n",
        "callback_list.append(checkpoint_callback)\n",
        "callback_list.append(TensorBoard(log_dir=\"logs\", \n",
        "                        histogram_freq=0, \n",
        "                        write_graph=True, \n",
        "                        write_images=False))\n",
        "\n",
        "cascade_model = model_object.model\n",
        "if using_gpu:\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        cascade_model.compile(optimizer=model_object.optimizer, loss=model_object.loss, metrics=[\"accuracy\"])\n",
        "        cascade_model.fit(x_train,y_train,validation_data=(x_val,y_val),callbacks = callback_list, epochs = epochs, batch_size = batch_size)\n",
        "        #Saving weights\n",
        "        # model.save_weights('weightsfile-multiclass2.h5')\n",
        "else:\n",
        "    cascade_model.compile(optimizer=model_object.optimizer, loss=model_object.loss, metrics=[\"accuracy\"])\n",
        "    cascade_model.summary()\n",
        "    cascade_model.fit(x_train,y_train,validation_data=(x_val,y_val), epochs = epochs, batch_size = batch_size)\n",
        "\n",
        "\n",
        "\n",
        "cascade_model.save_weights('Testmy_model_weights_depth_5.h5', overwrite = True, save_format = 'h5')\n",
        "#cascade_model.load_weights('Testmy_model_weights.h5')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 20, 22, 10) (?, 20, 22, 10)\n",
            "(?, 20, 22, 10) (?, 20, 22, 10)\n",
            "(?, 20, 22, 10) (?, 20, 22, 10)\n",
            "(?, 20, 22, 10) (?, 20, 22, 10)\n",
            "(?, 20, 22, 10) (?, 20, 22, 10)\n",
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input1 (InputLayer)             [(None, 20, 22, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input2 (InputLayer)             [(None, 20, 22, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input3 (InputLayer)             [(None, 20, 22, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input4 (InputLayer)             [(None, 20, 22, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input5 (InputLayer)             [(None, 20, 22, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 20, 22, 10)   260         input1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 20, 22, 10)   260         input2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 20, 22, 10)   260         input3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 20, 22, 10)   260         input4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 20, 22, 10)   260         input5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_200 (Dropout)           (None, 20, 22, 10)   0           conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_206 (Dropout)           (None, 20, 22, 10)   0           conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_212 (Dropout)           (None, 20, 22, 10)   0           conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_218 (Dropout)           (None, 20, 22, 10)   0           conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_224 (Dropout)           (None, 20, 22, 10)   0           conv2d_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 20, 22, 10)   2510        dropout_200[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 20, 22, 10)   2510        dropout_206[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 20, 22, 10)   2510        dropout_212[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 20, 22, 10)   2510        dropout_218[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 20, 22, 10)   2510        dropout_224[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_201 (Dropout)           (None, 20, 22, 10)   0           conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 20, 22, 10)   110         dropout_200[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_207 (Dropout)           (None, 20, 22, 10)   0           conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 20, 22, 10)   110         dropout_206[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_213 (Dropout)           (None, 20, 22, 10)   0           conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 20, 22, 10)   110         dropout_212[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_219 (Dropout)           (None, 20, 22, 10)   0           conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 20, 22, 10)   110         dropout_218[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_225 (Dropout)           (None, 20, 22, 10)   0           conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 20, 22, 10)   110         dropout_224[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_5 (TensorFlo [(None, 20, 22, 20)] 0           dropout_201[0][0]                \n",
            "                                                                 conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_8 (TensorFlo [(None, 20, 22, 20)] 0           dropout_207[0][0]                \n",
            "                                                                 conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_11 (TensorFl [(None, 20, 22, 20)] 0           dropout_213[0][0]                \n",
            "                                                                 conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_14 (TensorFl [(None, 20, 22, 20)] 0           dropout_219[0][0]                \n",
            "                                                                 conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_17 (TensorFl [(None, 20, 22, 20)] 0           dropout_225[0][0]                \n",
            "                                                                 conv2d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 20, 22, 10)   5010        tf_op_layer_concat_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 20, 22, 10)   5010        tf_op_layer_concat_8[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 20, 22, 10)   5010        tf_op_layer_concat_11[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 20, 22, 10)   5010        tf_op_layer_concat_14[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_217 (Conv2D)             (None, 20, 22, 10)   5010        tf_op_layer_concat_17[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_202 (Dropout)           (None, 20, 22, 10)   0           conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 20, 22, 10)   110         dropout_200[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 20, 22, 10)   110         dropout_201[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_208 (Dropout)           (None, 20, 22, 10)   0           conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 20, 22, 10)   110         dropout_206[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 20, 22, 10)   110         dropout_207[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_214 (Dropout)           (None, 20, 22, 10)   0           conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 20, 22, 10)   110         dropout_212[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 20, 22, 10)   110         dropout_213[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_220 (Dropout)           (None, 20, 22, 10)   0           conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 20, 22, 10)   110         dropout_218[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 20, 22, 10)   110         dropout_219[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_226 (Dropout)           (None, 20, 22, 10)   0           conv2d_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_218 (Conv2D)             (None, 20, 22, 10)   110         dropout_224[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_219 (Conv2D)             (None, 20, 22, 10)   110         dropout_225[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_6 (TensorFlo [(None, 20, 22, 30)] 0           dropout_202[0][0]                \n",
            "                                                                 conv2d_174[0][0]                 \n",
            "                                                                 conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_9 (TensorFlo [(None, 20, 22, 30)] 0           dropout_208[0][0]                \n",
            "                                                                 conv2d_185[0][0]                 \n",
            "                                                                 conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_12 (TensorFl [(None, 20, 22, 30)] 0           dropout_214[0][0]                \n",
            "                                                                 conv2d_196[0][0]                 \n",
            "                                                                 conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_15 (TensorFl [(None, 20, 22, 30)] 0           dropout_220[0][0]                \n",
            "                                                                 conv2d_207[0][0]                 \n",
            "                                                                 conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_18 (TensorFl [(None, 20, 22, 30)] 0           dropout_226[0][0]                \n",
            "                                                                 conv2d_218[0][0]                 \n",
            "                                                                 conv2d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 20, 22, 10)   7510        tf_op_layer_concat_6[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 20, 22, 10)   7510        tf_op_layer_concat_9[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 20, 22, 10)   7510        tf_op_layer_concat_12[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 20, 22, 10)   7510        tf_op_layer_concat_15[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_220 (Conv2D)             (None, 20, 22, 10)   7510        tf_op_layer_concat_18[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_203 (Dropout)           (None, 20, 22, 10)   0           conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 20, 22, 10)   110         dropout_200[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 20, 22, 10)   110         dropout_201[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 20, 22, 10)   110         dropout_202[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_209 (Dropout)           (None, 20, 22, 10)   0           conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 20, 22, 10)   110         dropout_206[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 20, 22, 10)   110         dropout_207[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 20, 22, 10)   110         dropout_208[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_215 (Dropout)           (None, 20, 22, 10)   0           conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 20, 22, 10)   110         dropout_212[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 20, 22, 10)   110         dropout_213[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 20, 22, 10)   110         dropout_214[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_221 (Dropout)           (None, 20, 22, 10)   0           conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 20, 22, 10)   110         dropout_218[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 20, 22, 10)   110         dropout_219[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 20, 22, 10)   110         dropout_220[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_227 (Dropout)           (None, 20, 22, 10)   0           conv2d_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_221 (Conv2D)             (None, 20, 22, 10)   110         dropout_224[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_222 (Conv2D)             (None, 20, 22, 10)   110         dropout_225[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_223 (Conv2D)             (None, 20, 22, 10)   110         dropout_226[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_7 (TensorFlo [(None, 20, 22, 40)] 0           dropout_203[0][0]                \n",
            "                                                                 conv2d_177[0][0]                 \n",
            "                                                                 conv2d_178[0][0]                 \n",
            "                                                                 conv2d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_10 (TensorFl [(None, 20, 22, 40)] 0           dropout_209[0][0]                \n",
            "                                                                 conv2d_188[0][0]                 \n",
            "                                                                 conv2d_189[0][0]                 \n",
            "                                                                 conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_13 (TensorFl [(None, 20, 22, 40)] 0           dropout_215[0][0]                \n",
            "                                                                 conv2d_199[0][0]                 \n",
            "                                                                 conv2d_200[0][0]                 \n",
            "                                                                 conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_16 (TensorFl [(None, 20, 22, 40)] 0           dropout_221[0][0]                \n",
            "                                                                 conv2d_210[0][0]                 \n",
            "                                                                 conv2d_211[0][0]                 \n",
            "                                                                 conv2d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_19 (TensorFl [(None, 20, 22, 40)] 0           dropout_227[0][0]                \n",
            "                                                                 conv2d_221[0][0]                 \n",
            "                                                                 conv2d_222[0][0]                 \n",
            "                                                                 conv2d_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 20, 22, 20)   20020       tf_op_layer_concat_7[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 20, 22, 20)   20020       tf_op_layer_concat_10[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 20, 22, 20)   20020       tf_op_layer_concat_13[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 20, 22, 20)   20020       tf_op_layer_concat_16[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_224 (Conv2D)             (None, 20, 22, 20)   20020       tf_op_layer_concat_19[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_204 (Dropout)           (None, 20, 22, 20)   0           conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_210 (Dropout)           (None, 20, 22, 20)   0           conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_216 (Dropout)           (None, 20, 22, 20)   0           conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_222 (Dropout)           (None, 20, 22, 20)   0           conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_228 (Dropout)           (None, 20, 22, 20)   0           conv2d_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_35 (Flatten)            (None, 8800)         0           dropout_204[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_36 (Flatten)            (None, 8800)         0           dropout_210[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_37 (Flatten)            (None, 8800)         0           dropout_216[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_38 (Flatten)            (None, 8800)         0           dropout_222[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_39 (Flatten)            (None, 8800)         0           dropout_228[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_49 (Dense)                (None, 40)           352040      flatten_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_50 (Dense)                (None, 40)           352040      flatten_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_51 (Dense)                (None, 40)           352040      flatten_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_52 (Dense)                (None, 40)           352040      flatten_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_53 (Dense)                (None, 40)           352040      flatten_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_205 (Dropout)           (None, 40)           0           dense_49[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_211 (Dropout)           (None, 40)           0           dense_50[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_217 (Dropout)           (None, 40)           0           dense_51[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_223 (Dropout)           (None, 40)           0           dense_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_229 (Dropout)           (None, 40)           0           dense_53[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 200)          0           dropout_205[0][0]                \n",
            "                                                                 dropout_211[0][0]                \n",
            "                                                                 dropout_217[0][0]                \n",
            "                                                                 dropout_223[0][0]                \n",
            "                                                                 dropout_229[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_7 (Tenso [(None, 1, 200)]     0           concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   (None, 10)           8440        tf_op_layer_ExpandDims_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_54 (Dense)                (None, 40)           440         lstm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_55 (Dense)                (None, 4)            164         dense_54[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,949,094\n",
            "Trainable params: 1,949,094\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 455984 samples, validate on 113992 samples\n",
            "Epoch 1/20\n",
            "455984/455984 [==============================] - 183s 401us/sample - loss: 0.0750 - acc: 0.9687 - val_loss: 3.9749e-04 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "455984/455984 [==============================] - 179s 392us/sample - loss: 0.0186 - acc: 0.9932 - val_loss: 0.0059 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "455984/455984 [==============================] - 179s 393us/sample - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0636 - val_acc: 0.9843\n",
            "Epoch 4/20\n",
            "455984/455984 [==============================] - 180s 395us/sample - loss: 0.0039 - acc: 0.9989 - val_loss: 3.7578e-05 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "455984/455984 [==============================] - 180s 395us/sample - loss: 0.0066 - acc: 0.9979 - val_loss: 3.1925e-05 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "455984/455984 [==============================] - 181s 396us/sample - loss: 0.0030 - acc: 0.9991 - val_loss: 4.3325e-05 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "455984/455984 [==============================] - 181s 397us/sample - loss: 0.0024 - acc: 0.9993 - val_loss: 1.3161e-05 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "455984/455984 [==============================] - 180s 395us/sample - loss: 0.0051 - acc: 0.9985 - val_loss: 2.8554e-05 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "455984/455984 [==============================] - 180s 395us/sample - loss: 0.0022 - acc: 0.9994 - val_loss: 5.3237e-05 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "455984/455984 [==============================] - 180s 395us/sample - loss: 0.0017 - acc: 0.9995 - val_loss: 5.4435e-05 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "455984/455984 [==============================] - 181s 396us/sample - loss: 0.0012 - acc: 0.9996 - val_loss: 2.5032e-05 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "455984/455984 [==============================] - 181s 396us/sample - loss: 0.0028 - acc: 0.9993 - val_loss: 0.0045 - val_acc: 0.9999\n",
            "Epoch 13/20\n",
            "455984/455984 [==============================] - 181s 397us/sample - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0012 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "455984/455984 [==============================] - 181s 397us/sample - loss: 0.0018 - acc: 0.9995 - val_loss: 3.2243e-05 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "455984/455984 [==============================] - 181s 397us/sample - loss: 0.0018 - acc: 0.9995 - val_loss: 8.2558e-05 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "455984/455984 [==============================] - 181s 398us/sample - loss: 0.0028 - acc: 0.9993 - val_loss: 0.0031 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "455984/455984 [==============================] - 182s 398us/sample - loss: 0.0016 - acc: 0.9996 - val_loss: 9.2136e-05 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "455984/455984 [==============================] - 181s 397us/sample - loss: 0.0022 - acc: 0.9994 - val_loss: 6.5403e-05 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "455984/455984 [==============================] - 182s 398us/sample - loss: 7.2717e-04 - acc: 0.9998 - val_loss: 1.4308e-05 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "455984/455984 [==============================] - 181s 397us/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 5.7791e-05 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVOiwAQQTbe7",
        "colab_type": "code",
        "outputId": "206216e9-ec1d-4148-9eec-a9c2b0f3a4d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "window_size = 5\n",
        "cnn_activation =\"relu\"\n",
        "hidden_activation=\"relu\"\n",
        "model_activation=\"softmax\"\n",
        "pool_size = (1,1)\n",
        "number_conv2D_filters = 10\n",
        "kernel_shape = (5)\n",
        "number_lstm_cells = 10\n",
        "number_nodes_hidden = 40\n",
        "loss = \"categorical_crossentropy\"\n",
        "optimizer = \"adam\"\n",
        "using_gpu = False\n",
        "model_object = CascadeMulti(window_size,cnn_activation, hidden_activation, model_activation, pool_size,\n",
        "                            number_conv2D_filters, kernel_shape, number_lstm_cells, number_nodes_hidden, \n",
        "                            loss, optimizer,using_gpu)\n",
        "\n",
        "\n",
        "cascade_model = model_object.model\n",
        "cascade_model.compile(optimizer=model_object.optimizer, loss=model_object.loss, metrics=[\"accuracy\"])\n",
        "\n",
        "cascade_model.load_weights('Testmy_model_weights.h5')#loading best weights from exp7"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "(?, 20, 22, 10) (?, 20, 22, 10)\n",
            "(?, 20, 22, 10) (?, 20, 22, 10)\n",
            "(?, 20, 22, 10) (?, 20, 22, 10)\n",
            "(?, 20, 22, 10) (?, 20, 22, 10)\n",
            "(?, 20, 22, 10) (?, 20, 22, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-57db9bce6452>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcascade_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mcascade_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testmy_model_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#loading best weights from exp7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    160\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    161\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1422\u001b[0m         \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m         \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    735\u001b[0m                      \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                      \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                      ' layers.')\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m   \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 28 layers into a model with 63 layers."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t233efxLTzQC",
        "colab_type": "code",
        "outputId": "b4de0f3f-5c02-47e4-bf79-e65f91075a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# import custom_data_download as utils\n",
        "import os\n",
        "#subjects = ['100307', '102816', '105923', '106521', '108323', '109123', '111514', '112920', '113922', '116726', '133019', '140117', '146129']\n",
        "# subjects_ismail = ['105923']\n",
        "\n",
        "# subjects_ismail = ['164636','105923','162935']\n",
        "# subjects_ismail = ['133019','164636']\n",
        "# subjects_ismail = ['164636']\n",
        "# subjects_ismail = ['105923','162935']\n",
        "subjects_ismail = ['164636' ,'156334']\n",
        "\n",
        "\n",
        "\n",
        "#subjects_ismail = ['100307','102816','105923' ]\n",
        "personal_access_key_id = \"AKIAXO65CT57MAX2EWPN\"\n",
        "secret_access_key = \"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\"\n",
        "\n",
        "create_data_directory()\n",
        "# subjects_ismail = utils.get_filtered_subjects(personal_access_key_id,secret_access_key,subjects_ismail)\n",
        "download_batch_subjects(subjects_ismail, personal_access_key_id, secret_access_key, os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating data directory or skipping if already existing...\n",
            "Creating data directory or skipping if already existing...\n",
            "Creating the directories for the subject '164636'\n",
            "\n",
            "done !\n",
            "\n",
            "Will start downloading the following files for all folders:\n",
            "['c,rfDC', 'config', 'e,rfhp1.0Hz,COH', 'e,rfhp1.0Hz,COH1']\n",
            "\n",
            "\n",
            "downloading c,rfDC file for folder 3-Restin ...\n",
            "\n",
            "done downloading c,rfDC for folder 3-Restin !\n",
            "\n",
            "downloading c,rfDC file for folder 4-Restin ...\n",
            "\n",
            "done downloading c,rfDC for folder 4-Restin !\n",
            "\n",
            "downloading c,rfDC file for folder 5-Restin ...\n",
            "\n",
            "done downloading c,rfDC for folder 5-Restin !\n",
            "\n",
            "downloading c,rfDC file for folder 6-Wrkmem ...\n",
            "\n",
            "done downloading c,rfDC for folder 6-Wrkmem !\n",
            "\n",
            "downloading c,rfDC file for folder 7-Wrkmem ...\n",
            "\n",
            "done downloading c,rfDC for folder 7-Wrkmem !\n",
            "\n",
            "downloading c,rfDC file for folder 8-StoryM ...\n",
            "\n",
            "done downloading c,rfDC for folder 8-StoryM !\n",
            "\n",
            "downloading c,rfDC file for folder 9-StoryM ...\n",
            "\n",
            "done downloading c,rfDC for folder 9-StoryM !\n",
            "\n",
            "downloading c,rfDC file for folder 10-Motort ...\n",
            "\n",
            "done downloading c,rfDC for folder 10-Motort !\n",
            "\n",
            "downloading c,rfDC file for folder 11-Motort ...\n",
            "\n",
            "done downloading c,rfDC for folder 11-Motort !\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/164636/unprocessed/MEG/3-Restin/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 775459  =      0.000 ...   381.153 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 775460)\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/164636/unprocessed/MEG/6-Wrkmem/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 1310770  =      0.000 ...   644.268 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 1310771)\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/164636/unprocessed/MEG/8-StoryM/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 943543  =      0.000 ...   463.769 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 943544)\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/164636/unprocessed/MEG/10-Motort/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 1490019  =      0.000 ...   732.372 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 1490020)\n",
            "\n",
            "done creating the compressed h5 files for subject '164636' !\n",
            "\n",
            "deleting the directory containing the binary files of subject '164636' ...\n",
            "\n",
            "Done deleting the directory of the binary files!\n",
            "Moving on to the next subject ...\n",
            "\n",
            "Creating the directories for the subject '156334'\n",
            "\n",
            "done !\n",
            "\n",
            "Will start downloading the following files for all folders:\n",
            "['c,rfDC', 'config', 'e,rfhp1.0Hz,COH', 'e,rfhp1.0Hz,COH1']\n",
            "\n",
            "\n",
            "downloading c,rfDC file for folder 3-Restin ...\n",
            "\n",
            "done downloading c,rfDC for folder 3-Restin !\n",
            "\n",
            "downloading c,rfDC file for folder 4-Restin ...\n",
            "\n",
            "done downloading c,rfDC for folder 4-Restin !\n",
            "\n",
            "downloading c,rfDC file for folder 5-Restin ...\n",
            "\n",
            "done downloading c,rfDC for folder 5-Restin !\n",
            "\n",
            "downloading c,rfDC file for folder 6-Wrkmem ...\n",
            "\n",
            "done downloading c,rfDC for folder 6-Wrkmem !\n",
            "\n",
            "downloading c,rfDC file for folder 7-Wrkmem ...\n",
            "\n",
            "done downloading c,rfDC for folder 7-Wrkmem !\n",
            "\n",
            "downloading c,rfDC file for folder 8-StoryM ...\n",
            "\n",
            "done downloading c,rfDC for folder 8-StoryM !\n",
            "\n",
            "downloading c,rfDC file for folder 9-StoryM ...\n",
            "\n",
            "done downloading c,rfDC for folder 9-StoryM !\n",
            "\n",
            "downloading c,rfDC file for folder 10-Motort ...\n",
            "\n",
            "done downloading c,rfDC for folder 10-Motort !\n",
            "\n",
            "downloading c,rfDC file for folder 11-Motort ...\n",
            "\n",
            "done downloading c,rfDC for folder 11-Motort !\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/156334/unprocessed/MEG/3-Restin/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 762670  =      0.000 ...   374.867 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 762671)\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/156334/unprocessed/MEG/6-Wrkmem/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 1288237  =      0.000 ...   633.193 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 1288238)\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/156334/unprocessed/MEG/8-StoryM/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 947197  =      0.000 ...   465.565 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 947198)\n",
            "\n",
            "Reading the binary file and returning the raw matrix ...\n",
            "Reading 4D PDF file /content/drive/My Drive/Brain_Data/156334/unprocessed/MEG/10-Motort/4D/c,rfDC...\n",
            "Creating Neuromag info structure ...\n",
            "... Setting channel info structure.\n",
            "... no headshape file supplied, doing nothing.\n",
            "Currently direct inclusion of 4D weight tables is not supported. For critical use cases please take into account the MNE command \"mne_create_comp_data\" to include weights as printed out by the 4D \"print_table\" routine.\n",
            "Current compensation grade : 0\n",
            "Reading 0 ... 1481087  =      0.000 ...   727.982 secs...\n",
            "\n",
            "Creating the compressed h5 files ...\n",
            "\n",
            "shape of raw matrix (248, 1481088)\n",
            "\n",
            "done creating the compressed h5 files for subject '156334' !\n",
            "\n",
            "deleting the directory containing the binary files of subject '156334' ...\n",
            "\n",
            "Done deleting the directory of the binary files!\n",
            "Moving on to the next subject ...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIPfNXGGZEbk",
        "colab_type": "code",
        "outputId": "778f4410-2809-408e-fa72-4d7f05b5b8fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "training_file_dir = \"Data/train\"\n",
        "all_train_files = [f for f in listdir(training_file_dir) if isfile(join(training_file_dir, f))]\n",
        "train_files_dirs = []\n",
        "for i in range(len(all_train_files)):\n",
        "    train_files_dirs.append(training_file_dir+'/'+all_train_files[i])\n",
        "\n",
        "validation_file_dir = \"Data/validate\"\n",
        "all_validate_files = [f for f in listdir(validation_file_dir) if isfile(join(validation_file_dir, f))]\n",
        "validate_files_dirs = []\n",
        "for i in range(len(all_validate_files)):\n",
        "    validate_files_dirs.append(validation_file_dir+'/'+all_validate_files[i])\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "callback_list = []\n",
        "\n",
        "gc.collect()\n",
        "x_train,y_train = load_data(train_files_dirs)\n",
        "print(\"done loading training !\")\n",
        "print(\"byte size of training data: {}\".format(sys.getsizeof(x_train)))\n",
        "x_val,y_val = load_data(validate_files_dirs)\n",
        "print(\"done loading validation !\")\n",
        "print(\"byte size of validation data: {}\".format(sys.getsizeof(x_val)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of files to load: 46\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 92624)\n",
            "(248, 92624)\n",
            "(248, 92624)\n",
            "(248, 92624)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 103454)\n",
            "(248, 103454)\n",
            "(248, 103454)\n",
            "(248, 103454)\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 92624)\n",
            "(248, 92624)\n",
            "(248, 92624)\n",
            "(248, 92624)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 103454)\n",
            "(248, 103454)\n",
            "(248, 103454)\n",
            "(248, 103454)\n",
            "rest part\n",
            "math part\n",
            "mem part\n",
            "motor part\n",
            "part 5 temp dicts\n",
            "part concatenation\n",
            "part reshaping\n",
            "part shuffling\n",
            "part custom normalization\n",
            "part final dict assignment\n",
            "done loading training !\n",
            "byte size of training data: 240\n",
            "number of files to load: 12\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 92624)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 103454)\n",
            "(248, 25791)\n",
            "(248, 25791)\n",
            "(248, 92624)\n",
            "(248, 33344)\n",
            "(248, 33344)\n",
            "(248, 103454)\n",
            "rest part\n",
            "math part\n",
            "mem part\n",
            "motor part\n",
            "part 5 temp dicts\n",
            "part concatenation\n",
            "part reshaping\n",
            "part shuffling\n",
            "part custom normalization\n",
            "part final dict assignment\n",
            "done loading validation !\n",
            "byte size of validation data: 240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8YHmG2Rle--",
        "colab_type": "code",
        "outputId": "8a7b3d8f-7ffa-45d4-e2b1-7ed95eba6a4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 10\n",
        "#105923 108323 140117 109123 '106521', '133019' '164636' ,'156334'\n",
        "cascade_model.compile(optimizer=model_object.optimizer, loss=model_object.loss, metrics=[\"accuracy\"])\n",
        "cascade_model.summary()\n",
        "cascade_model.fit(x_train,y_train,validation_data=(x_val,y_val), epochs = epochs, batch_size = batch_size)\n",
        "\n",
        "\n",
        "\n",
        "cascade_model.save_weights('Testmy_model_weights.h5', overwrite = True, save_format = 'h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input1 (InputLayer)             [(None, 20, 22, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input2 (InputLayer)             [(None, 20, 22, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input3 (InputLayer)             [(None, 20, 22, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input4 (InputLayer)             [(None, 20, 22, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input5 (InputLayer)             [(None, 20, 22, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 20, 22, 10)   500         input1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 20, 22, 10)   500         input2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 20, 22, 10)   500         input3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 20, 22, 10)   500         input4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 20, 22, 10)   500         input5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 20, 22, 10)   0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 20, 22, 10)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 20, 22, 10)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 20, 22, 10)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 20, 22, 10)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 20, 22, 10)   4910        dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 20, 22, 10)   4910        dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 20, 22, 10)   4910        dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 20, 22, 10)   4910        dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 20, 22, 10)   4910        dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 20, 22, 10)   0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 20, 22, 10)   110         dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 20, 22, 10)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 20, 22, 10)   110         dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 20, 22, 10)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 20, 22, 10)   110         dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 20, 22, 10)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 20, 22, 10)   110         dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 20, 22, 10)   0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 20, 22, 10)   110         dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat (TensorFlowO [(None, 20, 22, 20)] 0           dropout_1[0][0]                  \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1 (TensorFlo [(None, 20, 22, 20)] 0           dropout_5[0][0]                  \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2 (TensorFlo [(None, 20, 22, 20)] 0           dropout_9[0][0]                  \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3 (TensorFlo [(None, 20, 22, 20)] 0           dropout_13[0][0]                 \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_4 (TensorFlo [(None, 20, 22, 20)] 0           dropout_17[0][0]                 \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 20, 22, 20)   19620       tf_op_layer_concat[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 20, 22, 20)   19620       tf_op_layer_concat_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 20, 22, 20)   19620       tf_op_layer_concat_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 20, 22, 20)   19620       tf_op_layer_concat_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 20, 22, 20)   19620       tf_op_layer_concat_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 20, 22, 20)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 20, 22, 20)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 20, 22, 20)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 20, 22, 20)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 20, 22, 20)   0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 8800)         0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 8800)         0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 8800)         0           dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 8800)         0           dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 8800)         0           dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 40)           352040      flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 40)           352040      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 40)           352040      flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 40)           352040      flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 40)           352040      flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 40)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 40)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 40)           0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 40)           0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 40)           0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 200)          0           dropout_3[0][0]                  \n",
            "                                                                 dropout_7[0][0]                  \n",
            "                                                                 dropout_11[0][0]                 \n",
            "                                                                 dropout_15[0][0]                 \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims (TensorF [(None, 1, 200)]     0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 10)           8440        tf_op_layer_ExpandDims[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 40)           440         lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 4)            164         dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,894,944\n",
            "Trainable params: 1,894,944\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 455984 samples, validate on 113992 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/10\n",
            "455984/455984 [==============================] - 93s 205us/sample - loss: 0.4014 - acc: 0.8318 - val_loss: 0.2004 - val_acc: 0.9961\n",
            "Epoch 2/10\n",
            "455984/455984 [==============================] - 83s 183us/sample - loss: 0.2150 - acc: 0.9163 - val_loss: 0.0386 - val_acc: 0.9990\n",
            "Epoch 3/10\n",
            "455984/455984 [==============================] - 83s 183us/sample - loss: 0.1006 - acc: 0.9628 - val_loss: 0.1383 - val_acc: 0.8742\n",
            "Epoch 4/10\n",
            "455984/455984 [==============================] - 90s 198us/sample - loss: 0.1044 - acc: 0.9608 - val_loss: 0.0202 - val_acc: 0.9977\n",
            "Epoch 5/10\n",
            "455984/455984 [==============================] - 83s 183us/sample - loss: 0.0578 - acc: 0.9785 - val_loss: 0.0017 - val_acc: 1.0000\n",
            "Epoch 6/10\n",
            "455984/455984 [==============================] - 89s 195us/sample - loss: 0.0438 - acc: 0.9840 - val_loss: 0.2878 - val_acc: 0.8743\n",
            "Epoch 7/10\n",
            "455984/455984 [==============================] - 84s 184us/sample - loss: 0.0405 - acc: 0.9853 - val_loss: 4.1113e-04 - val_acc: 1.0000\n",
            "Epoch 8/10\n",
            "455984/455984 [==============================] - 90s 197us/sample - loss: 0.0237 - acc: 0.9915 - val_loss: 0.0447 - val_acc: 0.9823\n",
            "Epoch 9/10\n",
            "455984/455984 [==============================] - 84s 183us/sample - loss: 0.0189 - acc: 0.9933 - val_loss: 3.6078e-04 - val_acc: 1.0000\n",
            "Epoch 10/10\n",
            "455984/455984 [==============================] - 88s 194us/sample - loss: 0.0182 - acc: 0.9936 - val_loss: 0.0022 - val_acc: 0.9997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjdjKu-CGGO0",
        "colab_type": "code",
        "outputId": "6b5992d3-ce24-4bd5-c5e6-aa208d1c7e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "'''\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "def get_checkpoint_max_accuracy(directory):\n",
        "    files = glob.glob(directory+\"/*.hdf5\")\n",
        "    print(\"Found {} files in the directory {}\".format(len(files),directory))\n",
        "    accuracies = []\n",
        "    for file in files:\n",
        "        temp1 = file.split('_')\n",
        "        last_part = temp1[-1]\n",
        "        parts = last_part.split('.')\n",
        "        first2 = parts[:2]\n",
        "        accuracy_string = \".\".join(item for item in first2)\n",
        "        accuracies.append(float(accuracy_string))\n",
        "        \n",
        "    index_max = np.argmax(accuracies)\n",
        "    file_max_accuracy = files[index_max]\n",
        "    print(\"The file with the max accuracy is file {}\".format(file_max_accuracy))\n",
        "    return file_max_accuracy\n",
        "\n",
        "# best_weights_exp1 = get_checkpoint_max_accuracy(\"/content/drive/My Drive/Experiments/Cascade/Experiment2/checkpoints\")\n",
        "# best_weights_exp26 = get_checkpoint_max_accuracy(\"/content/drive/My Drive/Experiments/Cascade/Experiment10/checkpoints\")\n",
        "# best_weights_exp26 = get_checkpoint_max_accuracy(\"/content/drive/My Drive/NewExperiment2/checkpoints\")\n",
        "best_weights_exp26 = get_checkpoint_max_accuracy(\"/content/drive/My Drive/Experiments/Cascade/Experiment76/checkpoints\")\n",
        "print(best_weights_exp26)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 0 files in the directory /content/drive/My Drive/Experiments/Cascade/Experiment76/checkpoints\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-046a917efcd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# best_weights_exp26 = get_checkpoint_max_accuracy(\"/content/drive/My Drive/Experiments/Cascade/Experiment10/checkpoints\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# best_weights_exp26 = get_checkpoint_max_accuracy(\"/content/drive/My Drive/NewExperiment2/checkpoints\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mbest_weights_exp26\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_checkpoint_max_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Experiments/Cascade/Experiment76/checkpoints\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_weights_exp26\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-046a917efcd6>\u001b[0m in \u001b[0;36mget_checkpoint_max_accuracy\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mindex_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mfile_max_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_max\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The file with the max accuracy is file {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_max_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \"\"\"\n\u001b[0;32m-> 1153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Q0xN0IYlbU",
        "colab_type": "code",
        "outputId": "6c0f4a95-3674-46bf-d189-c4a15281d9a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys\n",
        "\n",
        "#THIS ONE\n",
        "#import ModelCascadeMulti as cascade\n",
        "test_file_dir = \"Data/test\"\n",
        "all_test_files = [f for f in listdir(test_file_dir) if isfile(join(test_file_dir, f))]\n",
        "test_files_dirs = []\n",
        "for i in range(len(all_test_files)):\n",
        "    test_files_dirs.append(test_file_dir+'/'+all_test_files[i])\n",
        "\n",
        "\n",
        "x_test,y_test = load_data(test_files_dirs)\n",
        "print(\"done loading testing !\")\n",
        "print(\"byte size of testing data: {}\".format(sys.getsizeof(x_test)))\n",
        "#best_checkpoint_weights_exp7 = get_checkpoint_max_accuracy(\"Experiments/Cascade/Experiment7/checkpoints\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "window_size = 5\n",
        "cnn_activation =\"relu\"\n",
        "hidden_activation=\"relu\"\n",
        "model_activation=\"softmax\"\n",
        "pool_size = (1,1)\n",
        "number_conv2D_filters = 10\n",
        "kernel_shape = (5)\n",
        "number_lstm_cells = 10\n",
        "number_nodes_hidden = 40\n",
        "loss = \"categorical_crossentropy\"\n",
        "optimizer = \"adam\"\n",
        "using_gpu = False\n",
        "model_object = CascadeMulti(window_size,cnn_activation, hidden_activation, model_activation, pool_size,\n",
        "                            number_conv2D_filters, kernel_shape, number_lstm_cells, number_nodes_hidden, \n",
        "                            loss, optimizer,using_gpu)\n",
        "\n",
        "\n",
        "cascade_model = model_object.model\n",
        "cascade_model.compile(optimizer=model_object.optimizer, loss=model_object.loss, metrics=[\"accuracy\"])\n",
        "\n",
        "cascade_model.load_weights('Testmy_model_weights_depth_5.h5')#loading best weights from exp7\n",
        "\n",
        "\n",
        "#Evaluation of the model with the test data set\n",
        "result = cascade_model.evaluate(x_test, y_test, batch_size = 128)\n",
        "prediction=cascade_model.predict(x_test)\n",
        "print(prediction.shape)\n",
        "print(result)\n",
        "gc.collect"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of files to load: 40\n",
            "rest_162026\n",
            "(248, 35624)\n",
            "rest_162026\n",
            "(248, 35624)\n",
            "rest_162026\n",
            "(248, 35624)\n",
            "rest_162026\n",
            "(248, 35624)\n",
            "rest_162026\n",
            "(248, 35624)\n",
            "rest_162026\n",
            "(248, 35624)\n",
            "rest_162026\n",
            "(248, 35624)\n",
            "rest_162026\n",
            "(248, 35624)\n",
            "rest_162026\n",
            "(248, 35624)\n",
            "rest_162026\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "(248, 35624)\n",
            "rest part\n",
            "math part\n",
            "mem part\n",
            "motor part\n",
            "part 5 temp dicts\n",
            "part concatenation\n",
            "part reshaping\n",
            "part shuffling\n",
            "part custom normalization\n",
            "part final dict assignment\n",
            "done loading testing !\n",
            "byte size of testing data: 240\n",
            "(?, 20, 22, 10) (?, 20, 22, 10)\n",
            "(?, 20, 22, 10) (?, 20, 22, 10)\n",
            "(?, 20, 22, 10) (?, 20, 22, 10)\n",
            "(?, 20, 22, 10) (?, 20, 22, 10)\n",
            "(?, 20, 22, 10) (?, 20, 22, 10)\n",
            "284988/284988 [==============================] - 44s 153us/sample - loss: 2.3975 - acc: 0.7515\n",
            "(284988, 4)\n",
            "[2.3975350249616256, 0.7514843]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function gc.collect>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSGwiZiLXQIZ",
        "colab_type": "code",
        "outputId": "6fe0f1d7-3964-4c47-cc81-ed4775d66317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "!zip -r Experiment6_13_01.zip Experiments/Cascade/Experiment6\n",
        "from google.colab import files\n",
        "files.download(\"Experiment6_13_01.zip\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: Experiments/Cascade/Experiment6/ (stored 0%)\n",
            "updating: Experiments/Cascade/Experiment6/plot_model6.jpg (deflated 31%)\n",
            "updating: Experiments/Cascade/Experiment6/info_epochs_model6.txt (deflated 84%)\n",
            "updating: Experiments/Cascade/Experiment6/summary_model6.txt (deflated 34%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/ (stored 0%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_009-val_acc_0.932.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_011-val_acc_0.981.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_017-val_acc_0.983.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_018-val_acc_0.985.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_002-val_acc_0.727.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_008-val_acc_0.967.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_014-val_acc_0.866.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_012-val_acc_0.959.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_007-val_acc_0.983.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_013-val_acc_0.976.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_020-val_acc_0.962.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_005-val_acc_0.861.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_003-val_acc_0.738.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_006-val_acc_0.979.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_001-val_acc_0.620.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_015-val_acc_0.884.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_016-val_acc_0.984.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_004-val_acc_0.793.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_010-val_acc_0.915.hdf5 (deflated 11%)\n",
            "updating: Experiments/Cascade/Experiment6/checkpoints/checkpoint-epoch_019-val_acc_0.985.hdf5 (deflated 11%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNAix9fUYNCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !zip -r Experiment7_13_01.zip Experiments/Cascade/Experiment7\n",
        "# from google.colab import files\n",
        "files.download(\"Experiment7_13_01.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}